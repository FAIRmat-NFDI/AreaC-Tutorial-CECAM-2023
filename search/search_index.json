{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This tutorial provides an overview of using the NOMAD software, with a focus on support for classical molecular simulations. NOMAD is a free open-source data management platform for Materials Science which follows the F.A.I.R. (Findable, Accessible, Interoperable, and Reusable) principles <sup>1</sup><sup>2</sup>, and is developed primarily by the FAIRmat consortium.</p> <p>This tutorial was created by the FAIRmat Area C (computation) team:</p> <p>Dr. Nathan Daelman, nathan.daelman@physik.hu-berlin.de     - contact for electronic structure calculations</p> <p>Dr. Jos\u00e9 M. Pizarro, jose.pizarro@physik.hu-berlin     - contact for excited state calculations</p> <p>Dr. Joseph F. Rudzinski, joseph.rudzinski@physik.hu-berlin.de     - general contact, contact for molecular simulations</p> <ol> <li> <p>Mark D. Wilkinson et al. The fair guiding principles for scientific data management and stewardship. Scientific Data, 3(1):160018, Mar 2016. URL: https://doi.org/10.1038/sdata.2016.18, doi:10.1038/sdata.2016.18.\u00a0\u21a9</p> </li> <li> <p>GO FAIR: FAIR principles. URL: https://www.go-fair.org/fair-principles/.\u00a0\u21a9</p> </li> </ol>"},{"location":"Exercise_Answers/","title":"Exercise Answers","text":""},{"location":"Exercise_Answers/#exercise-answers","title":"Exercise Answers","text":"<ol> <li> <p>OW, HW1, and HW2 (see run \u2192 method \u2192 atom_parameters \u2192 atom_index \u2192 label)</p> </li> <li> <p>step = 5000, time = 5 ps (see system \u2192 10 \u2192 step and system \u2192 10 \u2192 time, respectively)</p> </li> <li> <p>thermostat = \"langevin_goga\", frequency of coupling = 500 fs (see workflow \u2192 molecular_dynamics \u2192 integration_parameters \u2192 thermostat_parameters \u2192 thermostat_type and workflow \u2192 molecular_dynamics \u2192 integration_parameters \u2192 thermostat_parameters \u2192 coupling_constant, respectively).</p> </li> </ol>"},{"location":"Exercise_Answers_2/","title":"Exercise Answers 2","text":""},{"location":"Exercise_Answers_2/#exercise-answers","title":"Exercise Answers","text":"<ol> <li> <p>This is an NVT annealing simulation to cool the system from ~1000 k to ~300 k. As the temperature cools, the molecules aggregate, as indicated by the growing peaks in the rdfs. However, the pressure remains low due to the (fixed) large box size.</p> </li> <li> <p>This is an NPT simulation, performed at ~300 k. Mid-way through the simulation there is a clear transition in the potential energy and an increase in the pressure, indicating that the box is contracting and the system is entering a proper liquid phase. This is validated through the rdfs, which become properly normalized to 1 at large distances later in the trajectory.</p> </li> <li> <p>This is a fully equilibrated NPT simulation.</p> </li> </ol>"},{"location":"Tutorial-1_Uploading_MD_Data/","title":"Uploading molecular dynamics data to NOMAD (~40 min)","text":"<p>There are several ways to upload your data to NOMAD:</p> <ul> <li>By dragging-and-dropping your files into the <code>PUBLISH &gt; Uploads</code> page.</li> <li>By using the shell command <code>curl</code> for sending files for upload.</li> <li>By using the python <code>response</code> library to submit a query to the NOMAD API.</li> </ul> <p>For this tutorial, we will stick to the simple drag-and-drop method. However, some information about the python API for uploading is provided under the <code>Advanced &gt; Using python API for uploading</code>.</p> <p>In general, you can upload files one by one or upload entire file structures in <code>.zip</code> or <code>.tar.gz</code> formats. First, download the zip file with the example simulation data for this part of the tutorial:</p> <p> Download Test Data </p> <p>Take a minute to examine the directory structure. If you are familiar with Gromacs you will immediately see the input/output from 3 simulations: an energy minimization (<code>Emin/</code>), an NPT equilibration (<code>Equil-NPT/</code>), and an NVT production run (<code>Prod-NVT/</code>). In the main directory, you will also see a .yaml file, which contains the NOMAD schema for connecting these 3 simulations into a workflow. More information about these custom workflow schemas can be found under <code>Advanced &gt; Creating custom workflows</code>.</p> <p>On the top-left menu, click on <code>PUBLISH &gt; Uploads</code>.</p> <p>Then click on <code>CREATE A NEW UPLOAD</code> and either drag-and-drop the <code>water_workflow.zip</code> file directly onto the page or click on the <code>CLICK OR DROP FILES</code> button to find it in your local directories.</p> <p>After the files are uploaded, a processing is triggered. In brief, NOMAD interprets the files and divides them into two categories: mainfiles and auxiliary files. In the same upload, there might be multiple mainfiles and auxiliary files organized in a folder tree structure.</p> Info <p>The mainfiles are those files which are representative of a given computational calculation. The presence of a mainfile in the upload is required for NOMAD to recognize a calculation. NOMAD supports several computational codes for first principles calculations, molecular dynamics simulations, and lattice modeling, as well as workflow and database managers. Currently, both the Gromacs and Lammps packages are supported. We are also developing a custom schema based on the H5MD format, to allow users to upload simulation data run with any MD engine.</p> <p>For each supported code, NOMAD recognizes a single file as the mainfile. For example, the Gromacs mainfile is the native <code>.log</code> file created during the simulation. The remaining files that have not been identified as mainfiles are designated as auxiliary files. You can find further information about the various supported codes, mainfiles, and auxiliary files in the general NOMAD documentation under Supported parsers.</p> Tip <p>We recommend to keep as many auxiliary files as possible together with the mainfile, but without exceeding the uploads limit\u201432GB file size limit per upload. For the routine upload of simulations that exceed this limit, we suggest that you prune the trajectory file in advance, to store only a subset of the data on the NOMAD repository. In this case, the parsers should still correctly store the configurations as well as additional metadata dealing with the input parameters to the simulation.</p> <p>For special cases of larger simulations or datasets that must be stored in full, special processing procedures are required. In this case, you should contact the NOMAD/FAIRmat team for assistance. Alternatively, you can post questions or requests on the NOMAD MATSCI Community Discourse Forum.</p> <p>During the processing, NOMAD will store the simulation data and metadata within the NOMAD Metainfo schema. In this case, the parsing should take ~ 30 seconds. You should now see the successfully processed data overview:</p>"},{"location":"Tutorial-1_Uploading_MD_Data/#sections-of-the-uploads-page","title":"Sections of the Uploads page","text":"<p>At the top of the uploads page, you can modify certain general metadata fields.</p> <p>The name of the upload can be modify by clicking on the pen icon . The other icons correspond to:</p> <ul> <li> Manage members: allows users to invite collaborators by defining co-authors and reviewers roles.</li> <li> Download files: downloads all files present in the upload.</li> <li> Reload: reloads the uploads page.</li> <li> Reprocess: triggers again the processing of the uploaded data.</li> <li> API: generates a query with a JSON body to use by the NOMAD API.</li> <li> Delete the upload: deletes completely the upload.</li> </ul> <p>The remainder of the uploads page is divided in 4 sections. The first section, (1) Prepare and upload your files, shows the files and folder structure in the upload. You can add a <code>README.md</code> in the root directory and its content will be shown above this section..</p> <p>We will skip section 2 for now and come back to it in a second.</p> <p>The third section, (3) Edit author metadata, allows users to edit certain metadata fields from all entries recognized in the upload. This includes comments, where you can add as much extra information as you want, references, where you can add a URL to your upload (e.g., an article DOI), and datasets, where you can create or add the uploaded data into a more general dataset.</p> <p> </p> <p>The final section, (4) Publish_, lets the user to publish the data with or without an embargo. This will be explained more in detail in How-to publish data.</p> Warning <p>Please, do not publish this test data! There is a test deployment of NOMAD where you can safely perform test publishing, as the data is periodically deleted.</p> <p>Now go back to the second section, (2) Process data, which shows the processed data and the generated entries in NOMAD:</p> <p>Let's examine the production simulation by clicking the 3 dots to the right of the entry labeled <code>Prod-NVT/mdrun_Prod-NVT.log</code>, circled in green in the above image. You will now be on the OVERVIEW page for this entry, which aims to provide a simple description of this entry through visualizations of the system itself, some key observables, and some of the overarching metadata. The OVERVIEW page will be examined in detail in Tutorial 2. For now, we will focus on how the uploaded data is stored within the NOMAD repository. In addition to the OVERVIEW tab, there are 3 other tabs at the top of the page: FILES, DATA, and LOGS.</p> <p>Click on the FILES tab. Here you will find all the raw data that was uploaded via the .zip file, retained within the original file system structure. The raw files are stored in the repository and can be downloaded at any time.</p> <p>Now click on the LOGS tab. Here you will find some technical information about the data processing along with any warnings or errors that were raised by the NOMAD software.</p> <p>Finally, click on the DATA tab. Here you can navigate through the NOMAD Metainfo for this entry, i.e., the processed and normalized version of the simulation data and metadata.</p>"},{"location":"Tutorial-1_Uploading_MD_Data/#the-nomad-metainfo","title":"The NOMAD Metainfo","text":"<p>NOMAD stores all processed data in a well defined, structured, and machine readable format, known as the <code>archive</code>. The schema that defines the organization of (meta)data within the archive is known as the <code>MetaInfo</code>. More information can be found in the NOMAD docs: An Introduction to Schemas and Structured Data in NOMAD.</p> <p>Duplicate your tab and go to <code>Analyze &gt; The NOMAD Metainfo</code> in the top-left menu of NOMAD. Here you can navigate through or search the entire set of NOMAD Metainfo definitions.</p> <p>The NOMAD Metainfo covers a very wide range of materials data beyond computational data, including electronic lab notebooks and a variety of experimental techniques. The most important archive sections for computational data is illustrated in the following diagram:</p> <pre><code>archive\n\u251c\u2500\u2500 run\n\u2502  \u00a0 \u251c\u2500\u2500 method\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 atom_parameters\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 dft\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 forcefield\n\u2502  \u00a0 \u2502      \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u251c\u2500\u2500 system\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 atoms\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2502     \u251c\u2500\u2500 positions\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2502     \u251c\u2500\u2500 lattice_vectors\n\u2502  \u00a0 \u2502      \u2502     \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u2502      \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u2514\u2500\u2500 calculation\n\u2502  \u00a0        \u251c\u2500\u2500 energy\n\u2502  \u00a0        \u251c\u2500\u2500 forces\n\u2502  \u00a0        \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 workflow2\n \u00a0\u00a0  \u251c\u2500\u2500 method\n \u00a0\u00a0  \u251c\u2500\u2500 inputs\n \u00a0\u00a0  \u251c\u2500\u2500 tasks\n \u00a0\u00a0  \u251c\u2500\u2500 outputs\n \u00a0\u00a0  \u2514\u2500\u2500 results\n</code></pre> <p>Search through the NOMAD Metainfo sections displayed above to get an idea of what kinds of quantities are stored in each section. In Part II, you will learn how to explore the actual populated quantities of the Metainfo for specific entries.</p> <p>Navigate to section run program, where we find some basic information about the simulation code (name and version):</p> <p>Now navigate to section run method:</p> <p>Under atom_parameters you will find a list of force-field-dependent atom attributes such as mass and charge. Under force_field model contributions you will find a list of intramolecular interactions defined for this simulation. The development of metadata for a more comprehensive storage of force fields in NOMAD is currently underway. Under force_field force_calculations you will find some basic input parameters dealing with the calculation of forces in this simulation.</p> <p>Navigate to section run system 0:</p> <p>The system section holds the configurational information from this entry. The first item in this section, 0, corresponds to the first saved configuration within the simulation trajectory. Now click on the atoms section:</p> <p>There will be a prompt to ask if you would like to visualize the system. If you click yes, the particles within the simulation box will be displayed. (The visualizer can also be accessed with additional features in the OVERVIEW page). In the atoms section, various atom attributes are stored, e.g., the positions and velocities of each atom for this frame. Notice that the simulation trajectory is stored as a list of repeating dictionaries, as opposed to the typical dictionary of lists that may be found in trajectory analysis software, such as MDAnalysis. In short, this is done for consistency with data from other methods that are stored in the NOMAD repository. However, when working with the data from a particular entry, there are tools in NOMAD to easily convert the Metainfo to a more convenient format for analysis. More about this in Tutorial 3.</p> <p>Now click on the atoms_group section:</p> <p>This section holds a hierarchical organization of the system (denoted in the following as the topology), based on the bonds defined in the force field used for this simulation. By default, the first level of this hierarchy contains molecule groups, which group together all molecules of the same type. Now, by clicking the atoms_group subsection of this group, you reach the next level down, which contains each molecule within the current molecule group. Similarly to these molecule groups and molecule levels, monomer groups and monomer levels will be defined for polymer systems. The individual atoms within each molecule are not explicitly stored within this hierarchy, but are simply referenced via their indices within the atoms section. Take a few minutes to examine this hierarchy and the stored quantities in more detail. Note that at the moment the hierarchy is only stored in the first entry of system.</p> <p>Now go back and navigate to section run calculation 0:</p> <p>The calculation section contains any saved thermodynamic quantities that are a function of a single configuration, e.g., energy, pressure, temperature, etc., as well as any saved force information for the atoms within each configuration.</p> <p>Assignment</p> <ol> <li> <p>What are the oxygen and hydrogen atom types used in the force field for this simulation?</p> </li> <li> <p>What is the step number of the last saved configuration of this simulation? What is the corresponding time for this configuration?</p> </li> <li> <p>(CHALLENGE) Which thermostat is used for temperature coupling in this simulation? What is the frequency of temperature coupling?</p> </li> </ol> Success <ol> <li> <p>OW, HW1, and HW2 (see run \u2192 method \u2192 atom_parameters \u2192 atom_index \u2192 label)</p> </li> <li> <p>step = 5000, time = 5 ps (see system \u2192 10 \u2192 step and system \u2192 10 \u2192 time, respectively)</p> </li> <li> <p>thermostat = \"langevin_goga\", frequency of coupling = 500 fs (see workflow \u2192 molecular_dynamics \u2192 integration_parameters \u2192 thermostat_parameters \u2192 thermostat_type and workflow \u2192 molecular_dynamics \u2192 integration_parameters \u2192 thermostat_parameters \u2192 coupling_constant, respectively).</p> </li> </ol>"},{"location":"Tutorial-2_Overview_Page_and_Worfklow_Visualizer/","title":"The Molecular dynamics overview page and workflow visualizer (~40 min)","text":"<p>In this section, we will explore futher the features of the overview page. Let's first find some MD data to investigate. Go to <code>Explore &gt; Entries</code> from the top-left menu on the main (beta) NOMAD page. Go to the <code>Molecular Dynamics</code> tab on the <code>Filters</code> menu (left-hand side) and select <code>Temperature</code> under Available Properties. Now, go to the <code>Author / Origin / Data</code> filter tab. Under <code>Dataset Name</code>, select <code>Atomistic Molecular Dynamics Simulations of Pure Liquid...</code>. Now under results, select any entry in this dataset by clicking the  to the right of the entry.</p> <p>You are now on the overview page of the selected entry, which is split into several \"cards\" with information about the simulation stored in this entry. At the top is the <code>Material</code> card, which contains a visualization of the simulation box along with some basic composition and cell (i.e., box) information. The visualizer also contains a \"topology toggle bar\". The MD parsers in NOMAD use the bond information from the force field to create a hierarchy of molecular and chemical-fragment organization for the system. In this way, we quickly understand the molecular composition and physical state of the system.</p> <p>The <code>Original</code> tab of the topology bar displays information about the entire system. Now click on the <code>Group_MOL</code> tab. Notice some molecules have been made transparent in the visualizer. This group represents all molecules called <code>MOL</code> in the simulation box. Similarly, the <code>Group_MON</code> tab represents groups of molecules called <code>MON</code>. By toggling between these tabs, we can see that this simulation is a binary mixture that is well-mixed. Notice also that the <code>Composition</code> table below the visualizer updates when different topology bar tabs are selected. Underneath each of these molecular groups, you will find a tab with the respecitve name of each molecule. When you click these tabs, a single representative molecule will appear to show the molecular structure.</p> <p>The next card for these simulations is for <code>Thermodynamic properties</code>. Here we find plots of the time trajectories of various thermodynamic quantities: temperature, pressure, and potential energy in this case. Based on this information, what kind of simulation is contained within this entry (e.g., NVT, NPT, Simulated Annealing, etc.)?</p> <p>Now, scroll down to the <code>Structural properties</code> card. Once a molecular dynamics workflow is detected, the NOMAD software automatically tries to calculate radial distribution functions (rdfs) as a function of the molecular center of mass for each unique pair of molecule types. These rdfs are determined for various intervals of the trajectory, as a zeroth order measure of equilibration. Depending on the what type of simulation you have chosen, you may find that the rdfs are changing substantially during the simulation or are fully converged.</p> <p>Scroll down to the <code>Dynamical properties</code> card. Similar to the rdfs, NOMAD also calculates the molecular mean squared displacements (msds) for each molecule type. (Note that this requires at least 50 time frames present in the trajectory). A simple linear fitting procedure over the entire resulting msd curve is performed to determine the diffusion constant (displayed in the legend along with the corresponding Pearson correlation coefficient).</p> Tip <p>For processing efficiency, the calculation of rdfs and msds will be skipped for molecule groups containing more than 50k molecules, and the calculation of msds additionally require at least 50 trajectory frames.</p> <p>Scoll down to the <code>Workflow Graph</code> card. Here you find an interactive workflow graph that illustrates the workflow for this entry (i.e., a serial MD simulation). For workflows with many steps, only the first and last few tasks will be displayed. There are a number of known workflows, such as MD simulations, that are recognized by the NOMAD parsers, which then populate the workflow section of the NOMAD archive automatically.</p> Tip <p>Clicking on the individual tasks will take you inside of that task to show the corresponding inputs and outputs. Use the back button to return to the entire workflow. Clicking on the text labels will take you to the location where this information is stored within the NOMAD archive (i.e., in the Data tab of this entry).</p> <p>In addition to these standard workflows, users can also upload custom workflows, which can be used to connect various uploaded entries. You can examine an example of this by scrolling down to the <code>Entry References</code> card. Click on \"Referenced by the following entries\". You should see a single entry called \"workflow_archive.yaml\". Go to this entry by clicking the  to the right.</p> <p>You are now on the overview page for a custom workflow that contains the previous entry under investigation. By clicking the <code>Files</code> tab at the top of the page, and then navigating to <code>preview</code>, you can examine the <code>yaml</code> file that stores the workflow information. To learn more about creating such custom workflows, follow <code>Advanced &gt; Creating custom workflows</code>.</p> <p>On the overview page, we see another workflow graph, which illustrates a linear workflow consisting of a Geometry Opimization (i.e., energy minimization), followed by a series of 4 MD simulations.</p>"},{"location":"Tutorial-2_Overview_Page_and_Worfklow_Visualizer/#exercise","title":"Exercise","text":"<p>Investigate each of the entries contained in this workflow and provide a description of the corresponding simulation/calculation (for example, which ensemble? what temperature? is the system equilibrated? what phase is the system in?).</p> Tip <p>You can navigate to the individual overview pages by clicking the text label above each workflow task.</p> Success <ol> <li> <p>GeometryOptimization - An energy minimization of nearly 8k steps. The system is in the gas phase (see visualization). Only the end configuration is stored.</p> </li> <li> <p>MolecularDynamics #1 - A high temperature NVT simulation of the mixture in the gas phase (see also rdfs now) at 1000k.</p> </li> <li> <p>MolecularDynamics #2 - An NVT annealing simulation to cool the system from ~1000 k to ~300 k. As the temperature cools, the molecules aggregate, as indicated by the growing peaks in the rdfs. However, the pressure remains low due to the (fixed) large box size.</p> </li> <li> <p>MolecularDynamics #3 - An NPT simulation, performed at ~300 k. Midway through the simulation there is a clear transition in the potential energy and an increase in the pressure, indicating that the box is contracting and the system is entering the liquid phase. This is validated through the rdfs, which become properly normalized to 1 at large distances later in the trajectory.</p> </li> <li> <p>MolecularDynamics #4 - A fully equilibrated NPT simulation.</p> </li> </ol>"},{"location":"Tutorial-3_Extracting_Data_and_Trajectory_Analysis/","title":"Working with the NOMAD archive (~40 min)","text":"<p>In this part of the tutorial we will demonstrate how to query data from the NOMAD repository and work with it within a python environment.</p> <p>For this demonstration, we will utilize some tools from the nomad-lab software, as well as some third-party software (e.g., MDAnalysis, nglview).</p> <p>We suggest creating a virtual environment for this purpose. For example, using <code>conda</code>, you can set up the appropriate environment by first downloading the environment.yml file:</p> <p> Download environment.yml </p> <p>Then create a new conda environment with the following command:</p> <pre><code>conda env create -f environment.yml\n</code></pre> Tip <p>In case you have problems, here are the original commands used to set up the environment:</p> <pre><code>conda create -n \"CECAM_tutorial\" python==3.9\nconda activate CECAM_tutorial\npip install nomad-lab\nconda install nglview\nconda install \"ipywidgets &lt;8\" -c conda-forge\n</code></pre> <p>Or if you are using <code>virtualenv</code>:</p> <p> Download requirements.txt </p> <pre><code>python3 -m venv .pyenv\nsource .pyenv/bin/activate\npip install -r requirements.txt\n</code></pre> <p>warning</p> <p>You may have to restart your code editor (IDE) to get the in-notebook visualizations to work.</p> info <p>We stress that none of these packages are required to work with the NOMAD archive data. As you will see below, the archive data will be retrieved in a dictionary format, which you are free to work with in a plain python environment.</p> <p>Now, start a Jupyter notebook to carry out the remainder of this part of the tutorial.</p> <p>Import all the necessary modules:</p> <pre><code># Python\nimport numpy as np\n# timing\nimport time as t\n# I/O\nimport json\n# NOMAD API\nimport requests\n# NOMAD tools\nfrom nomad.atomutils import archive_to_universe\nfrom nomad.atomutils import BeadGroup\nfrom nomad.datamodel import EntryArchive\nfrom nomad.units import ureg\n# Visualization\nimport matplotlib.pyplot as plt\nimport nglview as nv\n# MDAnalysis\nimport MDAnalysis.analysis.rdf as MDA_RDF\nfrom MDAnalysis.analysis.distances import self_distance_array, distance_array\n</code></pre>"},{"location":"Tutorial-3_Extracting_Data_and_Trajectory_Analysis/#downloading-entire-entry-archives","title":"Downloading entire entry archives","text":"<p>In general, you can use the NOMAD Application Programming Interface (API) to</p> <ul> <li>upload data (especially useful when you are producing data via a workflow)</li> <li>delete data</li> <li>query all kinds of (meta)data: processed (from the archive), raw (from the repository), large bundles, individual entries, etc</li> </ul> <p>Each functionality has its own url, i.e. endpoints. There are over 60 different endpoints, each specialized in their own little subtask. You can find the full overview over at the API dashboard. It is highly recommended to have this page open when writing queries. Lastly, you can also use the dashboard to try out queries on the fly.</p> <p>In this first exercise, we will download the processed molecular dynamics trajectory of an individual entry to perform our own visualization and analysis. For this, we will be using the <code>/entries/{entry_id}</code>/archive. The <code>{entry_id}</code> here is variable, which you can get from the NOMAD website (or other API queries). Note that there are 3 versions of this endpoint: one that only gives you the metadata, one that returns the entire archive (<code>/download</code>), and a last wildcard that we will get into later (<code>/query</code>). These latter 2 options will be demonstrated below.</p> <p>Let's imagine that we searched the NOMAD repository using the filter bar of the GUI, as demonstrated in Part I of the tutorial, and found a short simulation of an atomistic box of hexane molecules that we might want to reuse. Open the link for this entry for reference as we analyze the queried data.</p> <p>From the Overview page of the entry, copy the <code>entry_id</code> from the left-hand side where the entry metadata is displayed. Define a variable with this information:</p> <p><pre><code>entry_id = ## PLACE ENTRY_ID HERE\n</code></pre> We also need to define the API endpoint:</p> <pre><code>nomad_api_prefix = 'https://nomad-lab.eu/prod/v1/api/v1/'\n</code></pre> <p>To download the entire archive for the entry of interest, we only have to execute a single command, and then we set the response to the variable <code>data</code>:</p> <pre><code>response = requests.get(nomad_api_prefix + 'entries/' + entry_id + '/archive/download')\ndata = response.json()\n</code></pre> <p>warning<p>The download may take 5 minutes or more, depending on your Internet's bandwidth.</p> </p> Info <p>Python provides a module for packaging and sending requests, aptly named <code>requests</code>. It comes with the methods <code>put</code>, <code>delete</code>, <code>get</code>, and <code>post</code>. Notice how the API dashboard lists each supported function next to its endpoint. The first 3 exactly serve the functionalities that we listed abovee (upload, delete, download). The last one, <code>post</code>, we will get into later.</p> <p>The formatted URL itself actually suffices to start the download. It just needs an interface. If you click on it (don't, it's just a hypothetical), your OS should open a browser to start the download. From the command line, you can use <code>curl</code>. The <code>requests</code> module is simply our Python interface. As with any https protocol, you will receive a status code. Additional information is transmitted in a JSON format (another web standard), which we deserialize to a dictionary here. When everything is successful, this contains the data we were looking for. In case of an error, the NOMAD API uses it to better articulate the reason under the keyword <code>detail</code>. For the full format we again refer the reader to the API dashboard.</p> Tip <p>Here is a more thorough set of functions for executing the download, while timing the download, and printing out some response info:</p> <pre><code>nomad_api_prefix = 'https://nomad-lab.eu/prod/v1/api/v1/'\ndef nomad_individual_archive_url(entry_id: str, endpoint_type: str=''):\n'''Produces the endpoint URL for downloading a NOMAD individual archive.\n    `entry_id` specifies the entry ID of the particular archive you want to download.\n    Use `endpoint_type` to further specify the particular subtype (`download` or `query`).'''\nendpoint_specifications = ('download', 'query')\nendpoint = f'{nomad_api_prefix}/entries/{entry_id}/archive'\nif endpoint_type :\nif endpoint_type in endpoint_specifications:\nendpoint += f'/{endpoint_type}'\nelse:\nraise ValueError(f'endpoint_type must be one of {endpoint_specifications}')\nreturn endpoint\n</code></pre> <pre><code>def measure_method(method, *args, **kwargs):\n\"\"\"\n    Measure the execution time of a given method with arguments.\n    Args:\n        method: The method/function to be measured.\n        *args: Positional arguments to be passed to the method.\n        **kwargs: Keyword arguments to be passed to the method.\n    \"\"\"\nstart_time = t.time()\nresult = method(*args, **kwargs)\nend_time = t.time()\nelapsed_time = end_time - start_time\nelapsed_minutes = int(elapsed_time // 60)\nelapsed_seconds = int(elapsed_time % 60)\nprint(f\"Method took {elapsed_minutes} minutes and {elapsed_seconds} seconds to execute.\")\nreturn result\n</code></pre> <pre><code># execute the download\nnomad_url = nomad_individual_archive_url('hxaepf6x12Xt2IX2jCt4DyfLG0P4', endpoint_type='download')\nresponse = measure_method(requests.get, nomad_url)\ndata = response.json()\n# print some of the responses\nprint(f'This is the endpoint URL: {nomad_url}. Click on it start downloading the archive via your browser.')\nprint(f\"This is the {response} message. In case of an error, check `response['detail']` to get the additional information.\")\nprint(f\"This is the top-level data structure of the deserialized message: {data.keys()}\")\n</code></pre> <pre><code>Method took 8 minutes and 47 seconds to execute.\nThis is the endpoint URL: https://nomad-lab.eu/prod/v1/api/v1//entries/hxaepf6x12Xt2IX2jCt4DyfLG0P4/archive/download. Click on it start downloading the archive via your browser.\nThis is the &lt;Response [200]&gt; message. In case of an error, check `response['detail']` to get the additional information.\nThis is the top-level data structure of the deserialized message: dict_keys(['processing_logs', 'run', 'workflow2', 'metadata', 'results', 'm_ref_archives'])\n</code></pre> <p>The keys of this dictionary corresponds one-to-one with the DATA tab of this entry's page:</p> <pre><code>print(data.keys())\n</code></pre> <pre><code>dict_keys(['processing_logs', 'run', 'workflow2', 'metadata', 'results', 'm_ref_archives'])\n</code></pre> <p>If you are interested in exploring the full schema with all its possible sections / quantities, check out the Metainfo Browser. You can use these representations of NOMAD's Metainfo schema to navigate the upcoming sections.</p> Tip <p>Here is an alternative way for viewing the data within your local python environment (although we recommend the DATA tab or Metainfo Browser):</p> <pre><code>def print_dict_as_tree(d, prefix=\"\", is_last=True):\n\"\"\"Function for printing a dictionary as a tree.\"\"\"\nkeys = list(d.keys())\nfor i, key in enumerate(keys):\nif i == len(keys) - 1:\nnew_prefix = prefix + \"\u2514\u2500 \"\nelse:\nnew_prefix = prefix + \"\u251c\u2500 \"\nvalue = d[key]\ntype_str = f\" ({type(value).__name__})\"\nlength_str = \"\"\nif isinstance(value, dict):\nprint(new_prefix + str(key) + type_str)\nis_last_child = i == len(keys) - 1\nprint_dict_as_tree(value, prefix + (\"    \" if is_last_child else \"\u2502   \"), is_last_child)\nelif isinstance(value, list):\nhas_dict = any(isinstance(item, dict) for item in value)\nif has_dict:\nprint(new_prefix + str(key) + type_str)\nfor item in value:\nif isinstance(item, dict):\nis_last_child = i == len(keys) - 1\nprint(prefix + (\"    \" if is_last_child else \"\u2502   \") + \"\u251c\u2500 [\")\nprint_dict_as_tree(item, prefix + (\"    \" if is_last_child else \"\u2502   \") + \"\u2502   \", True)\nprint(prefix + (\"    \" if is_last_child else \"\u2502   \") + \"\u2502   ]\")\nelse:\nis_last_child = i == len(keys) - 1\nlength_str = f\" (Length: {len(value)})\"\nelse:\nis_last_child = i == len(keys) - 1\nlength_str = f\" (Length: {len(value)})\"\nprint(new_prefix + str(key) + type_str + length_str)\nelse:\nprint(new_prefix + str(key) + type_str)\nif length_str:\nis_last_child = i == len(keys) - 1\nprint(prefix + (\"    \" if is_last_child else \"\u2502   \") + \"\u251c\u2500\" + length_str)\n</code></pre> <p>Assignment</p> <p>Get the atom positions for the first frame of the trajectory from this dictionary.</p> Success <pre><code>data['run'][0]['system'][0]['atoms']['positions']\n</code></pre>"},{"location":"Tutorial-3_Extracting_Data_and_Trajectory_Analysis/#using-tools-from-nomad-lab","title":"Using tools from nomad-lab","text":""},{"location":"Tutorial-3_Extracting_Data_and_Trajectory_Analysis/#the-nomad-archive-entry-format","title":"The NOMAD archive entry format","text":"<p>While it is perfectly acceptable to work directly with the archive dictionary, we can also convert this dictionary into a more sophisticated NOMAD archive object. This object supports unit conversion and a variety of ways for traversing the data tree.</p> <pre><code>archive = EntryArchive.m_from_dict(data)\n</code></pre> <p>We then define some easy access points for later use. You can see how the Python object style of navigating is exploited (i.e., using <code>.</code> instead of <code>['']</code>). Also pay close attention to when indices are used. They appear whenever a section is flagged as <code>repeats</code> in the Metainfo Browser.</p> <pre><code>section_run = archive.run[-1]\nsection_system = section_run.system\nsection_system_topology = section_run.system[0].atoms_group\nsection_atoms = section_system[0].atoms\n</code></pre> <p>Recall that the section run \u2192 system  is a list containing configurational information from each frame in the trajectory. Let's extract some information about the system, including positions, velocities, and box vectors:</p> <pre><code># get the number of atoms and frames, along with the atom names.\nn_atoms = section_atoms.get('n_atoms')\nn_frames = len(section_system) if section_system is not None else None\natom_names = section_atoms.get('labels')\n# get the atom positions, velocites, and box dimensions\npositions = np.empty(shape=(n_frames, n_atoms, 3))\nvelocities = np.empty(shape=(n_frames, n_atoms, 3))\ndimensions = np.empty(shape=(n_frames, 6))\nfor frame_ind, frame in enumerate(section_system):\nsec_atoms_fr = frame.get('atoms')\nif sec_atoms_fr is not None:\npositions_frame = sec_atoms_fr.positions\npositions[frame_ind] = ureg.convert(positions_frame.magnitude, positions_frame.units,\nureg.angstrom) if positions_frame is not None else None\nvelocities_frame = sec_atoms_fr.velocities\nvelocities[frame_ind] = ureg.convert(velocities_frame.magnitude, velocities_frame.units,\nureg.angstrom / ureg.picosecond) if velocities_frame is not None else None\nlatt_vec_tmp = sec_atoms_fr.get('lattice_vectors')\nif latt_vec_tmp is not None:\nlength_conversion = ureg.convert(1.0, sec_atoms_fr.lattice_vectors.units, ureg.angstrom)\ndimensions[frame_ind] = [\nsec_atoms_fr.lattice_vectors.magnitude[0][0] * length_conversion,\nsec_atoms_fr.lattice_vectors.magnitude[1][1] * length_conversion,\nsec_atoms_fr.lattice_vectors.magnitude[2][2] * length_conversion,\n90, 90, 90]  # nb -- for cubic box!\n</code></pre> <p>Assignment</p> <p>Fill in the missing variables assignments in the following code to make the temperature trajectory plot for this calculation. Compare your result to the plot from the Overview page in the NOMAD GUI.</p> <pre><code>fig = plt.figure(figsize=(10,4))\nsection_calculation =  ## FIND THE SECTION CALCULATION IN THE ARCHIVE ##\ntemperature = []\ntime = []\ntemperature_unit =  ## FIND THE UNIT OF TEMPERATURE USED IN THE ARCHIVE ##\ntime_unit =  ## FIND THE UNIT OF TIME USED IN THE ARCHIVE ##\nfor calc in section_calculation:\ntemperature.append()  ## FIND THE TEMPERATURE FOR THIS CALC ##\ntime.append()  ## FIND THE TIME FOR THIS CALC ##\nplt.plot(time, temperature)\nplt.ylabel(temperature_unit, fontsize=12)\nplt.xlabel(time_unit, fontsize=12)\nplt.show()\n</code></pre> Success <pre><code>    fig = plt.figure(figsize=(10,4))\nsection_calculation = archive.run[-1].calculation  ## FIND THE SECTION CALCULATION IN THE ARCHIVE ##\ntemperature = []\ntime = []\ntemperature_unit = section_calculation[0].temperature.units  ## FIND THE UNIT OF TEMPERATURE USED IN THE ARCHIVE ##\ntime_unit = section_calculation[0].time.units  ## FIND THE UNIT OF TIME USED IN THE ARCHIVE ##\nfor calc in section_calculation:\ntemperature.append(calc.temperature.magnitude)  ## FIND THE TEMPERATURE FOR THIS CALC ##\ntime.append(calc.time.magnitude)  ## FIND THE TIME FOR THIS CALC ##\nplt.plot(time, temperature)\nplt.ylabel(temperature_unit, fontsize=12)\nplt.xlabel(time_unit, fontsize=12)\nplt.show()\n</code></pre> <p>As you may have noticed, one of our bottlenecks in visualizing the trajectory is downloading the data. This is due to the archive's size (35.8 MB, not too uncommon with MD entries). It may be the case that before committing to downloading the entire archive for further analysis we want to examine a particular quantity, e.g., the temperature trajectory. This is a job for the <code>/query</code> endpoint. Go back to the API dashboard and check out its schema.</p> <p>This endpoint uses <code>post</code>, which is either used to add data (such as under <code>uploads</code>) or more complex queries and additional parameters, i.e. a more customizable <code>get</code> statement. This is in line with the standard semantics in HTTPS messages. In the case of our <code>/query</code> endpoint, the parameter we are looking for is <code>required</code> -the sections / quantities to be downloaded. Their specification follows the archive's tree structure as a nested dictionary. <code>requests</code> can append this information to the HTTPS body using the <code>json</code> argument. Below you will find the query specification. Pay attention to the optional use of indexes.</p> <pre><code>query_specification = {\n\"required\": {\n\"run[0]\": {\n\"calculation\": {\n\"temperature\": \"*\",\n\"time\": \"*\"\n}\n}\n}\n}\n</code></pre> <pre><code>response = requests.get(nomad_api_prefix + 'entries/' + entry_id + '/archive/query', json=query_specification)\ndata = response.json()\n</code></pre> Tip <p>Or using the functions provided in the previous tips: <pre><code>nomad_url_filtered = nomad_individual_archive_url('hxaepf6x12Xt2IX2jCt4DyfLG0P4', endpoint_type='query')\nresponse_filtered = measure_method(requests.post, nomad_url_filtered, json=query_specification)\ndata_filtered = response_filtered.json()\n</code></pre></p> <pre><code>Method took 0 minutes and 2 seconds to execute.\n</code></pre> <p>Note the difference in time! There is a noticeable speedup.</p> <p>The only disadvantage is that <code>EntryArchive</code> will fail with a partial archive. We can use still use the data as we would any dictionary to reproduce the same plot above. Just be careful to take care of unit conversions! To reintroduce units, multiply the quantity with its corresponding <code>ureg</code> unit, e.g. <code>ureg.second</code>. The quantities, as downloaded, are SI by default.</p> Tip <p>You can convert between units easily with the ureg module. For example, if a quantity <code>quantity_with_units</code> has units of seconds, you can convert to picoseconds with: <code>quantity_with_units = ureg.convert(quantity_with_units, quantity_with_units.units(), ureg.picoseconds)</code>.</p> <p>Now, take some time to search through the calculation section to see what other quantities are stored for this simulation.</p> <p>Assignment</p> <p>Now let's plot the center of mass molecular radial distribution function, averaged over the last 80% of the trajectory, as seeen in the Structural Properties card of the Overview page (red curve in plot). Fill in the missing variables assignments in the following code:</p> <pre><code>fig = plt.figure(figsize=(8,4))\nsection_MD =  ## FIND THE MOLECULAR DYNAMICS WORKFLOW SECTION IN THE ARCHIVE ##\nrdf_HEX_HEX =  ## FIND THE LAST HEX-HEX RDF STORED IN THE ARCHIVE ##\nrdf_start =  ## FIND THE STARTING FRAME FOR AVERAGING FOR THIS RDF ##\nrdf_end =  ## FIND THE ENDING FRAME FOR AVERAGING FOR THIS RDF ##\nbins = ureg.convert(rdf_HEX_HEX.bins.magnitude, rdf_HEX_HEX.bins.units, ureg.angstrom)\nplt.plot(bins, rdf_HEX_HEX.value)\nplt.xlabel(ureg.angstrom, fontsize=12)\nplt.ylabel('HEX-HEX rdf', fontsize=12)\nplt.xlim(0.1,15.0)\nplt.show()\n</code></pre> Success <pre><code>fig = plt.figure(figsize=(8,4))\nsection_MD = archive.workflow2  ## FIND THE MOLECULAR DYNAMICS WORKFLOW SECTION IN THE ARCHIVE ##\nrdf_HEX_HEX = section_MD.results.radial_distribution_functions[0].radial_distribution_function_values[-1]  ## FIND THE LAST HEX-HEX RDF STORED IN THE ARCHIVE ##\nrdf_start = rdf_HEX_HEX.frame_start  ## FIND THE STARTING FRAME FOR AVERAGING FOR THIS RDF ##\nrdf_end = rdf_HEX_HEX.frame_end  ## FIND THE ENDING FRAME FOR AVERAGING FOR THIS RDF ##\nbins = ureg.convert(rdf_HEX_HEX.bins.magnitude, rdf_HEX_HEX.bins.units, ureg.angstrom)\nplt.plot(bins, rdf_HEX_HEX.value)\nplt.xlabel(ureg.angstrom, fontsize=12)\nplt.ylabel('HEX-HEX rdf', fontsize=12)\nplt.xlim(0.1,15.0)\nplt.show()\n</code></pre> <p>Take some time to search through the workflow2 section to see what other quantities are stored there for this simulation.</p>"},{"location":"Tutorial-3_Extracting_Data_and_Trajectory_Analysis/#the-archive_to_universe-function","title":"The archive_to_universe function","text":"<p>We have seen above how to access quantities stored within a NOMAD archive entry using either the dictionary or NOMAD archive entry representation. This approach is sufficient for extracting singular quantities from the archive for further analysis.</p> <p>However, it would also be useful to have converters to store the archive for an MD simulation in a format more convenient to perform analysis. In particular, one may want to utilize existing analysis software to perform standard calculations. We have already implemented a converter to the MDAnalysis format. Run the following command to convert the archive into an MDAnalysis <code>universe</code>:</p> <pre><code>universe = archive_to_universe(archive)\n</code></pre> <p>Check which molecule types are present in the simulation:</p> <pre><code>print('Molecule Types')\nprint('--------------')\nfor moltype in np.unique(universe.atoms.moltypes):\nprint(moltype)\n</code></pre> <pre><code>Molecule Types\n--------------\nHEX\n</code></pre>"},{"location":"Tutorial-3_Extracting_Data_and_Trajectory_Analysis/#simple-analysis-with-mdanalysis","title":"Simple analysis with MDAnalysis","text":"<p>Let's calculate the center of mass rdf and compare with the plot stored in the archive. First define the appriate molecular groups:</p> <pre><code># Get an atom group for the Hexane molecules\nAG_HEX = universe.select_atoms('moltype HEX')\n# Create a \"bead group\" for the HEXANE molecules\nBG_HEX = BeadGroup(AG_HEX, compound=\"fragments\")\n# Define parameters for the rdf calculation\nmin_box_dimension = np.min(universe.trajectory[0].dimensions[:3])\nmax_rdf_dist = min_box_dimension / 2\nn_bins = 200\nn_smooth = 2\nn_prune = 1\n</code></pre> Tip <p>In MDAnalysis, it is not trivial to calculate center of mass rdfs. The concept of bead groups comes from a known work-around. This class is imported from the NOMAD software.</p> <p>Now run the rdf calculation using the MDAnalysis function <code>InterRDF</code>:</p> <pre><code># should take ~2 min\nexclusion_block = (1, 1)  # for removing self-distance\nrdf = MDA_RDF.InterRDF(\nBG_HEX, BG_HEX, range=(0, max_rdf_dist),\nexclusion_block=exclusion_block, nbins=n_bins).run(\nrdf_start, rdf_end, n_prune)\n</code></pre> <p>Smooth the rdf:</p> <pre><code>rdf.results.bins = rdf.results.bins[int(n_smooth / 2):-int(n_smooth / 2)]\nrdf.results.rdf = np.convolve(\nrdf.results.rdf, np.ones((n_smooth,)) / n_smooth,\nmode='same')[int(n_smooth / 2):-int(n_smooth / 2)]\n</code></pre> <p>Plot the rdf:</p> <pre><code>fig = plt.figure(figsize=(8,4))\nplt.plot(bins, rdf_HEX_HEX.value, label='NOMAD archive', color='k', lw=2)\nplt.plot(rdf.results.bins, rdf.results.rdf, label='MDAnalysis', linestyle='--', color='r', lw=2)\nplt.legend(fontsize=16)\nplt.xlabel(ureg.angstrom, fontsize=12)\nplt.ylabel('SOL-SOL rdf', fontsize=12)\nplt.xlim(0.1, 10.)\nplt.show()\n</code></pre> <p>Using MDAnalysis, we can also easily featurize the configurations for data-driven analysis. For example, calculate the pairwise distance matrix for carbons throughout the trajectory:</p> <pre><code>carbon_indices = [ind for ind, type in enumerate(universe.atoms.types) if type.startswith('C')]\nselection = 'index ' + ' '.join([str(ind) for ind in carbon_indices])\ncarbons_group = universe.select_atoms(f'{selection}')\nfor i_fr, frame in enumerate(universe.trajectory):\nif i_fr == 0:\ndistances = self_distance_array(carbons_group.positions, box=frame._unitcell)\nelse:\ndistances = np.vstack((distances, self_distance_array(carbons_group.positions, box=frame._unitcell)))\nprint(distances.shape)\n</code></pre> <pre><code>(11, 1282401)\n</code></pre>"},{"location":"Tutorial-3_Extracting_Data_and_Trajectory_Analysis/#visualization-using-nglviewer","title":"Visualization using NGLViewer","text":"<p>We can use the MDAnalysis representation along the the module <code>nglview</code> to visualize the trajectory within our notebook. First, unwrap the coordinates to make the molecules whole for visualization:</p> <pre><code>AG_all = universe.select_atoms('all')\nfor ts in universe.trajectory:\nAG_all.unwrap(compound='fragments')\n</code></pre> <p>Now, set up the viewer with a minimal representation: <pre><code>view = nv.show_mdanalysis(AG_all)\nview.center()\nview.clear()  # clear the initial representation automatically set up by nglview\nview.add_point('all')  # employ lightest rep\n# adjust the widget size\nview._set_size('700px', '600px')\nview\n</code></pre></p> <p>Then, you can adjust the visualization using nglview selection commands:</p> <pre><code>moltype = 'HEX'\nview.clear()\nview.add_point('all')\nselection = '@' + ', '.join([str(i) for i in universe.select_atoms('molnum 0')._ix])\nview.add_spacefill(selection)\n</code></pre>"},{"location":"part1/","title":"Part I: Overview of the NOMAD Archive and Repository (~60 min)","text":""},{"location":"part1/#introduction","title":"Introduction","text":"<p>In this tutorial, we will get you acquainted with the Graphical User Interface (GUI) in NOMAD, working our way through a couple of scenarios, similar to how you might encounter them in your daily activities. Each scenario starts with an assignment box explaining the objective and listing the main skills you will acquire. They then proceed with a step-by-step walkthrough, with the most essential steps highlighted in boldface.</p> <p>There are several checkpoints along the way to ensure you stay on track. These come in the shape of tips and success boxes, which you have to click to unfold. Additional useful information that might be straying too off-topic is listed in info boxes. Reading them is optional for completing your task, but might deliver useful extra insights. You can collapse them afterwards. When the information contains strategies for steering clear from mistakes, it will be labeled as warning. Each scenario assumes that you completed the previous ones, or that you are at least familiar with its objectives and terminology introduced.</p> <p>We start the tutorial with a brief overview of the NOMAD Archive and Repository (in short, NOMAD-lab).  A more general tutorial can be found in the FAIRmat tutorial 1.</p>"},{"location":"part1/#entries_section","title":"Navigating to the NOMAD","text":"<p>With the structure clear, let us jump into the NOMAD website. Only... there are several access points. The general landing page will give you a quick rundown of the NOMAD-lab, as wells as it features. Furthermore, it provides several links to documentation, tutorials, and the history behind the project.</p> <p>When accessing the data, however, we want to locate the NOMAD-lab itself. There are 2 public versions available:</p> <ol> <li>stable, as the default, under \"Open NOMAD\" (highlighted orange).</li> <li>beta /staging, which has the latest release and updates much more frequently. As such, it could also harbor unstable or untested features.</li> </ol> <p>Unless anything breaks, we recommend using the beta version. It has links at the bottom-right corner of the landing page, as well as under \"SOLUTIONS\" &gt; \"NOMAD\" &gt; \"Try and Test\" (highlighted red).</p> <p>To ensure the long-term reproducibility of this tutorial, we provide you with a link to the beta version, but capped at September 14th, 2023. Please visit this NOMAD page and take a look at its layout.</p> <p>As denoted at the top left, the page we have in front of us is called Entries. When loading the page, you should also see an orange box in the left-bottom corner, warning you that you are using an experimental product. You can get rid of it by clicking the check mark (<code>\u2713</code>).</p> <p>Some other layout elements worth mentioning are:</p> <ul> <li>in the middle, there is a whole list with tabulated data.</li> <li>just above it is a search bar with the text \"Type your query or keyword here\".</li> <li>on the left-hand side is a \"FILTERS\" side menu.</li> <li>at the very top is a header with several options (left-hand side), login and units (right-hand).</li> </ul> <p>Overall, the NOMAD Entries page is an intuitive, easy-to-use GUI. Here you can explore data according to your preferences by clicking on or typing out filters. As such, you can select from a variety of quantities that characterize the material or system under study, the methodology parameters followed in the calculation or experiment, and the output properties.</p> <p>Now, let us move on to the actual scenarios.</p>"},{"location":"part1/#scenario-1-example-setup","title":"Scenario 1 - Example setup","text":"<p>Assignment</p> <p>Imagine that you want to set up a Molecular Dynamics simulation of water in Gromacs, and would like some examples to help you get started. For example, you may want to compare results from several setups to find the one most suitable to you.</p> <p>In this exercise, you will learn how to:</p> <ul> <li>navigate the \"FILTERS\" side menu.</li> <li>toggle and combine filters.</li> <li>work with dynamic statistics.</li> </ul> <p>In the entries list NOMAD is already listing all database entries. The strategy is to narrow or filter this list down to our needs. Our main tool will be the side menu (left-hand side). Take stock of its structure. There is a list of themes or filter groups. Subgroups are indented below their main group. Any filter group with an arrow (<code>&gt;</code>) can be clicked open to reveal various filters and statistics in a side pane. To close the side pane and reveal the full table again, use the arrow back (<code>&lt;-</code>) in the top-left corner. Alternatively, you could also click anywhere outside of the pane, but be careful not to select anything unwanted.</p> <p>Now, scroll down to check out the full list. What main groups are there? Remember, we essentially want to learn more about Gromacs and its calculation setup. Under which filter groups do you think we could find them?</p> Success <p>While it is somewhat hidden, you can find the \"Program Name\" under \"Method\".</p> <p>The widget you see under this side pane has a double function: it acts both as a filter and a statistics overview. It comes with a text bar and a list of suggestions. You can try extending the list further by clicking \"SHOW MORE\", but Gromacs is not the most well-represented code in our database. Here, it is probably faster to just type out the program name and hit enter.</p> Tip <p>Almost all text bars in NOMAD support autocomplete. This comes in handy when you are unsure of the spelling or capitalization. Start with the first few letters and select your choice by clicking or hitting enter.</p> Success <p>You should be getting a result similar to the one in the reference picture. In the side menu, under \"Method\", you will find the active filter \"Program Name\" listed in grey, with its constraint / value \"GROMACS\" denoted in a blue, oval chip.</p> <p> </p> <p>Notice how the entries list changes with respect to your filter! Yet, the list of remaining calculations is still quite long (3.494 matches)... Most likely, there will not be a single filter that solves our problem completely. That is alright, though, we can just stack up several filters.</p> Tip <p>If the filters are not taking effect right away, click the redo button (<code>\u21ba</code>) next to \"FILTERS\". It will manually trigger an update of the entries list and statistics.</p> <p>Likewise, to reset all filters and start with a clean slate, click on the cancel button (<code>x</code>). Note that in the case of this tutorial, this also means removing the time constraint. Hence, if you decide to reset, the exact search results you see might start deviating from those in the tutorial. It is highly recommended to follow the guide.</p> <p>The appropriate filters depend on your goals and priorities. Since we are interested in running a molecular dynamics simulation, go to the filter subgroup called \"Molecular Dynamics\", and click it open. You are now presented with several settings. Judging from the statistics, we have the best chance with a setting that covers the widest range of calculations (28.000 entries). Select \"Temperature\" under \"Available Properties\". </p> Tip <p>If you have trouble understanding any term, just hover over it with the mouse. A summary text will appear. For example, \"Molecular Dynamics\" &gt; \"Available Properties\" clarifies that we are dealing with observables documented along the trajectory.</p> <p>System specifications are found under \"Material\". Since we know the composition of our system, click on \"Elements / Formula\". Notice how many of the elements in the periodic table are grayed out. This indicates that there are no entries containing these elements, given the filters that you have applied. Additionally, the number of entries containing each remaining element is displayed within the element's periodic table box, with a corresponding blue color gradient.</p> Tip <p>You can toggle the statistics scale via the dropdown menu to the right of the name. Conversely, if the statistics are ever slowing the browser down, deactivate them by deselecting \"Filters\" &gt; \"options menu\" (<code>\u22ee</code>) &gt; \"Show advanced statistics\".</p> <p>Now, select the elements contained in our system (we are looking for pure water, so O and H suffice). The corresponding chips will be added to the side menu, with an \"and\" in between. While filters between groups stack (i.e., \"and\" logic is applied), those within a side pane each have their own logic (more on that in scenario 2). Finally, remove all data with additional elements by selecting the \"only compositions that exclusively contain these atoms\" box.</p> Success <p>You are left with 2 MD calculations in the entries list. Clicking on the arrow (<code>-&gt;</code>) of whichever entry will bring you to its overview page. More on that in Part II - Overview page and workflows:. By clicking through on \"FILES\", you will get an overview of the uploaded files. These for sure contain output, but often also input. Both could serve as great starting points for deploying your own calculations.</p> <p> </p> <p>To save or share your active filters, you can simply bookmark the URL. We will also provide a solution button at the end of each exercise.</p> <p> Imtermittent solution </p> <p>The filters that we have chosen are just small questions or details that we use to construct an overall query. They can be added and removed, as we see fit. So, let us relax our conditions a bit and, leaving out the requirement for a temperature trajectory. Click the <code>x</code> next to the <code>temperature chip</code> or go to \"Molecular Dynamics\" &gt; \"Available Properties\" and click the highlighted checkbox again to deselect it. Now look at which entries are added. Are there any other filters that you would like to try out, or do you prefer checking them out by hand?</p> Success <p>Glossing over the remaining 6 results, 3 by Jannik Mehlis and 3 by Sebastian Baugmart, we retrieve 2 more MD simulations and 2 more \"Geometry Optimizations\". You may notice that some entries do not have a clear specification (under the \"Entry type\" column in the entries list).  These entries likely need to be reprocessed to classify them correctly, since the molecular dynamics support has been developed more recently. However, you can still find out their classification by visiting the entry's overview page.</p> <p>While geometry optimization was not part of our initial objective, these entries may inform us about the overarching workflow used by both authors, i.e., relaxing the system before the production simulation. Annotating such workflows is covered under <code>Advanced &gt; Creating custom workflows</code>.</p> <p> </p> <p> Final solution </p> Content in scenarios 2 and 3 <p>The upcoming scenarios do not deal with molecular dynamics data directly, due to lack of uploads. Nonetheless, they demonstrate useful features of the NOMAD repository, and hopefully motivate the increased use of NOMAD for molecular dynamics simulations.</p>"},{"location":"part1/#scenario-2-data-science","title":"Scenario 2 - Data science","text":"<p>Assignment</p> <p>You want to evaluate the impact of the metal used in Metal Organic Frameworks (MOFs). Maybe, if you find enough high-quality data, you can even train a machine-learned model. Specifically, you are interested in predicting the band gap <sup>1</sup>.</p> <p>In this exercise, you will learn how to:</p> <ul> <li>customize the entries table.</li> <li>use all 4 types of search bar queries.</li> <li>recognize \"OR\" filter stacking.</li> </ul> <p>Start a fresh session by clearing the molecular dynamics related filters, or by restoring the initial session.</p> <p>In this scenario our objective is more vaguely defined, so we will start by exploring the database before focusing in. A good overview is fundamental for spotting interesting data. As you will have noticed, our main tool here is the entries list (supplemented by the statistics). Unfortunately, the default columns (Entry \"Name\", the Hill \"Formula\", \"Entry type\", \"Upload time\", and \"Authors\") are not that helpful when exploring the materials space. To choose new columns, click on the three vertical slots (<code>|||</code>) in the upper-right corner, opposite to \"search results\". You will be presented with a checkbox menu of various quantities. Deselect</p> <ul> <li>\"Name\": it contains similar information as \"Formula\" and \"Entry Type\" combined,</li> <li>\"Upload time\": we do not care for now about when the data was uploaded,</li> <li>\"Author\": dito,</li> </ul> <p>and instead select</p> <ul> <li>\"Dimensionality\": to distinguish whether we are dealing with bulk, surface, or molecules.</li> <li>\"Crystal system\": the symmetry of the supercell.</li> <li>\"Space group symbol\": the symmetry of the atomic coordinates inside the supercell.</li> <li>\"Comment\": just to give us a bit more context, where possible.</li> </ul> <p>The 3 first selections are quantities who's filters can all be found under \"Material\" &gt; \"Structure\". Let us furthermore sort alphabetically by (Hill) \"Formula\" by clicking on the \"Formula\" heading. (Click multiple times to toggle between ascending / descending ordering).</p> What are entries exactly? <p>Entries are individually stored data packages, shown as rows in the overview table. In our context, they mostly overlap with an individual calculation, e.g., a single-point calculation or a single molecular dynamics run. When separate calculations are linked together into a workflow (see <code>Advanced &gt; Creating custom workflows</code>), the overall link also receives its own dedicated entry. Lastly, since NOMAD covers the whole of Condensed Matter Physics and Chemistry, entries can also be experimental samples or batches.</p> Success <p>You should now have a view in front of you similar to the reference figure. There is little room for deviation, since the horizontal column order is predetermined (matching the one in the selection box menu). Similarly, there can only be one column for sorting at a time.</p> <p> </p> <p>With our entries view all set up, we move on to the exploration part. More specifically, we will investigate the impact of several filters on our search. You can follow along with the suggestions here, but feel free to also try out on your own.</p> <p>While in the previous scenario we relied exclusively on the side menu, now that you are more familiar with the filters and their names, we can extend our toolkit with a faster alternative: the search bar. Its main purpose is to aid you in composing text-written filters and avoid having to switch between side panes all the time. The search bar does not, however, support the free-style natural language queries as found in web search engines, like Google, or AI models, like ChatGPT. The formatting here is far stricter. While you can switch back to the side menu at any time, we will , for educational purposes, rely solely on the search bar throughout this scenario.</p> Optimade <p>NOMAD also supports the Optimade API, which has its own query conventions (not covered in this tutorial). To use the NOMAD-Optimade endpoint, scroll down to \"Optimade\" at the very bottom of the side menu.</p> <p>We have a lot of leeway in which filters we tackle first. As usual, it is best to start with the attributes that are most clearly defined by our objective. In this case, it is that we are looking for MOFs. From there on we will follow the sequence: material; method; property, just as you would when generating your own data.</p> <p>Let us start again with the composition, or more specifically, by retrieving entries that contain carbon. Locate the search bar (above the entries list) and click into it to start typing. Try out a couple of keywords that come to your mind. As you are typing, NOMAD will autocomplete your query with several suggestions. Once you find a promising term, select it with the mouse or keyboard. Then write a single (not double or triple) equal sign (<code>=</code>) and fill in the value to filter for. Once you have it, press enter. Congratulations, you have applied your first equality query.</p> Tip <p>When searching for elements, do not fall for the mistake of writing out their name. NOMAD, and especially its search bar, aims for efficiency. So just stick to the elemental symbol from the periodic table. Lowercase also works.</p> What are these autocompleted filter names? <p>The full filter names that pop up in the suggestions are structured similarly to a filepath, but with dots (<code>.</code>) instead of slashes (<code>/</code> on Unix, <code>\\\\</code> on Windows). This is in line with the format of many other document databases. To explore this structure / schema, navigate to \"ANALYZE\" (in header) &gt; \"The NOMAD Metainfo\".</p> Success <p>You should have found the query \"results.material.elements=C\". Upon pressing enter, the same chip as usual appears in side menu, confirming that the filter is active. Moreover, note how the filter name in the side menu is contained in the the autocompleted version.</p> <p> </p> <p>As you saw, one can start out by writing the filter name, but you can just as well skip ahead to the value. Just type in  \"H\" and apply \"results.material.elements=H\". NOMAD automatically recognizes that you might mean an element, at which point it is easy to guess the matching filter. As such, we have refined our search to hydrocarbons, but MOFs also need ligands to bind the metal. Using the search bar, further stack oxygen and nitrogen filters. Note how the entries list changes.</p> <p>While the formulae are approaching what we are looking for, they do not look like MOFs quite yet. We already have enough elements for our skeleton / linkers, so let us just add a metal now. Unfortunately, the NOMAD filters do not know this concept. Instead, we will keep the last element a bit more open and just specify the number of elements. \"Number of\" is often abbreviated as \"n_\" in NOMAD. Type it into the search bar and select the appropriate filter name. We want to constrain the formula, but let us keep room for a wild card, e.g. another metal or ligand constituent. If it troubles us down the road, we can just tighten the filter. So finish the single inequality query with \"&lt;=6\".</p> <p>The current query will leave room for systems without any metal atom, i.e. not MOFs. Recreate the previous query, but hold off on pressing the enter key. Rather, you should constrain it with a lower limit as well. In particular, let's require at least 5 different elements. You can add a lower limit by placing your cursor at the beginning and writing a similar comparison. Like this, you have constructed the most complex search bar query, a double inequality query.</p> <p>Finally, we are interested in the material in bulk form specifically, no interfaces of any kind. Use the search bar to add this restriction.</p> Success <p>Your sandwiched LTE / GTE query should be either <code>4&lt;results.material.n_elements&lt;=6</code> or <code>5&lt;=results.material.n_elements&lt;=6</code>. Both yield the same results. Then you should also have added the equality query <code>results.material.structural_type=bulk</code>.</p> <p> </p> <p>Most columns seem much more uniform now. This was to be expected for \"Dimensionality\", since we explicitly enforced homogeneity, but \"Entry type\", \"Crystal system\" and \"Space group symbol\" are also affected. Even \"Comment\" seems to be following a repetitive format.</p> <p>Most importantly, the formulae only vary in metal contributions. These definitely look like MOFs. You can verify this by opening the entry overview of a row. Just click the arrow (<code>-&gt;</code>) right in the entries list. To return, use your browser's \"go back\" function.</p> <p>For machine-learning, a dataset should be homogeneous across its entire setup, safe for the variables that we are interested in. Most of the data on NOMAD is Density Functional Theory (DFT), with some GW and classical forcefields. GW would overall be better for high-quality band gaps, but DFT will end up being more useful due to its sheer number of entries. Just as with forcefields, DFT is mostly determined by the choice of kernel, i.e. density functional.</p> <p>Hybrid functionals are the norm for organic systems and the most popular in solid state by far are HSE06 and HSE03. By now, you probably have a good instinct of where to find them in the side menu (under \"DFT\"), but let us stick with the search bar for practice. Perform an equality query for both \"HSE03\" and \"HSE06\" (prominent hybrids in solid state).</p> Density functional nomenclature <p>The functional naming in NOMAD follows the convention established by libxc, a popular library for evaluating (semi)local functionals. In practice, this goes as <code>&lt;hybrid flag&gt;_&lt;Jacob's Ladder&gt;_&lt;exchange-correlation part&gt;_&lt;name identifier&gt;</code>, where <code>&lt;name identifier&gt;</code> is the main ID and the other tags simply provide metadata. <code>&lt;hybrid flag&gt;</code> is only present when the functional truly is a hybrid.</p> <p>Hold on. How can an entry contain 2 exchange-correlation functionals at once? Are we maybe filtering for workflows that contain both? For your answer, take a look at the side menu.</p> Success <p>Both \"HYB_GGA_XC_HSE03\" and \"HYB_GGA_XC_HSE06\" chips are present, but separated by the connector \"OR\" rather than \"AND\". Just as the name suggests, the logic condition is different in this case. Our \"XC Functional Names\" filter as not been narrowed down, but extend to search for both options.</p> <p> </p> <p> Note: In practice you would only choose a single functional when doing machine learning. Here, we look for 2 functionals just for educational reasons.</p> Multiple density functionals per entry <p>A single entry (and even calculation) may contain multiple functional names, just not <code>XC</code>! The libxc namely splits up exchange-correlation functionals (<code>XC</code>) into exchange (<code>X</code>) and correlation (<code>C</code>), when appropriate. So for example, the most prominent functional in NOMAD, <code>PBE</code>, is stored as <code>[GGA_X_PBE, GGA_C_PBE]</code>. Note that selecting one of either or even both (due to the <code>OR</code> logic), does not guarantee a user will retrieve only PBE.</p> <p>With the main method specified, there are still a bunch of additional numerical settings that may affect the fidelity of the results, such as the basis set. These can all be found under the filter subgroup \"Precision\". It is tough to estimate these parameters' actual impact. Therefore, they are best left till the end of the full query. Then you can evaluate the cost-benefit of reducing the dataset size for higher homogeneity or precision. For a full rundown on these newer features, feel free to check out FAIRmat Tutorial 10.</p> <p>Lastly, we only want data that contains the relevant observable, the band gap. Start by typing out \"band_gap\". Note how terms in the search bar never contain spaces, but use underscores (<code>_</code>) instead. Click on the relevant suggestion. If it does not fully match what you are looking for, feel free to shorten it until it does. To finish the presence queries, add \"=*\" and press enter.</p> Tip <p>To write an equality query for \"*\", use the escape character \"\\\", i.e. \"=\\*\". The escape character is not necessary for values containing \"*\", i.e. the radical \"CH3*\". Overall, there are very few instances of values containing \"*\" in NOMAD.</p> Success <p>The suggestions will present you with <code>results.properties.electronic.band_structure_electronic.band_gap.type</code> and <code>results.properties.electronic.band_structure_electronic.band_gap.value</code>. Both are a bit too deep down the search tree, since we are looking for <code>results.properties.electronic.band_structure_electronic.band_gap=*</code>.</p> <p>This filter yields a blank entries list. To understand why, examine the filter name: it targets only band gaps of band structure calculations. This is due to a legacy implementation, but has been mended. In the near future, you will be able to search for all reported band gaps.</p> <p>This filter stack is too restrictive. As a workaround, remove the last filter and let us go with an alternative. Formulate a presence query for the density of states, commonly abbreviated as DOS <sup>2</sup>.</p> Tip <p>The search bar uses lowercase for all quantities / filter names. Uppercase becomes relevant for values.</p> <p>Still, the search bar will include lower case suggestions of your spelling, where appropriate. To test this out, see what you get when typing \"DOS\".</p> Success <p>Your query should be <code>results.properties.electronic.dos_electronic=*</code> and return 2.833 entries. There is no need to narrow it down to spin-polarized calculations. We also accept spin-restricted data.</p> <p> </p> <p>From hereon, the best strategy would be to download the data you need, extract the band gap, and perform some statistical analysis first. You might come across some new ideas on how to further hone your query and filter out more noise! Click the checkbox next to the column headers in the entries list to select all entries. The 3 vertical slots now change to a download symbol, giving you the option between the original (raw) format or the NOMAD format (processed).</p> <p> </p> <p>Note that the specific analysis is not part of this tutorial. Other examples of analyzing NOMAD processed data are shown in part IV, however.</p> <p> Final solution </p> <p>What is in a URL?</p> <p>Column customization can at the moment not be saved between sessions.</p> <p>When stacking order matters</p> <p>Imagine having started filtering by property instead of composition and then method. You would have unwittingly excluded a vast dataset, potentially concluding that NOMAD does not host any suitable data. The general strategy to avoid working yourself in any of these dead ends is to start with broad filters, such as DOS instead of band gap. </p> <p>Also make sure to keep monitoring relevant indicators as you stack up filters. Even if you end up with unsatisfactory search results and start systematically removing filters, these indicators will be key in finding the best match. In the next scenario we will cover an even more powerful technique to aid in monitoring, dashboards.</p> <p>Conclusion: so-called query engineering is not just limited to Large Language Models, but also applies to sophisticated databases.</p>"},{"location":"part1/#scenario-3-finding-publications","title":"Scenario 3 - Finding Publications","text":"<p>Assignment</p> <p>You are talking to a colleague about your machine-learned model (from scenario 2). They tell you about a good recent research publication they saw by the author Rosen, but are forgetting the rest of the details at the moment. They will get back to you, but you are eager to check it out right away.</p> <p>In this exercise, you will learn how to:</p> <ul> <li>filter by publication metadata.</li> <li>set up a dashboard.</li> <li>examine an entry summary.</li> </ul> <p>Go back again to the initial session.</p> <p>The obvious starting point would be use a search engine specialized in publications, such as Google Scholar{:target=\"blank\"}. Just searching by the author's (last?) name, yields a suggestion for \"Robert A. Rose\", who seems to be working in biomedicine. Not quite what we were looking for... You can try adding some more terms describing the field, e.g. _ab initio, DFT or MOFs.</p> Success <p>This way it is possible to find the author's full name (Andrew S. Rosen) and also his publication history. Here we have hit a dead-end in as far as Google Scholar can help. Now it would be a matter of going over the publication list manually.</p> <p>Let us see how to leverage NOMAD for this research case. Note: in this scenario, we will walk you through using the side menu again. Of course, the filtering steps can also be executed via the search bar. Since we mention the filter names, it should not be hard for you to find their full names with autocomplete. Yet, the steps for setting up a dashboard do require opening up side panes.</p> <p>So, navigate to filter group \"Author / Origin / Dataset\", which covers publication metadata, near the bottom of the side menu. The first filter in the side pane is \"Author Name\". Type in \"Rosen\". We get 2 suggestions, but only one matches perfectly. Select \"Andrew Rosen\". Actually, if you performed the Google Scholar search successfully, you should have found the same author.</p> <p>Take stock of the \"Upload Create Time\" statistics right below \"Author Name\". It appears that Rosen is a researcher who has uploaded 3 times to NOMAD, each time in quite large batches ranging from to thousands to tens of thousands of entries. That is some very rich data. To better understand its makeup, we should be comparing several statistics at once. Jumping between side panes is a bit of a hassle, so instead we will speed up our analysis by setting up a dashboard. Click on the plus button (<code>+</code>) at the utmost right from \"Upload Create Time\" and return to the entries list.</p> <p>Tip</p> <p>If you cannot see all 3 upload times, it is most likely due to the binning. With zoom / autorange active, adjust the sliders on the x-axis to better encompass the time frames of interest.</p> Success <p>You should now find the same statistic nestled between the search bar and the entries list.</p> <p> </p> <p>This is our nascent dashboard. It will speed up our data exploration tremendously, but first we have to build it out a bit. Add to your dashboard:</p> <ul> <li>\"Author / Origin / Dataset\" &gt; \"Dataset Name\"</li> <li>the periodic table (\"Elements / Formula\" &gt; \"Elements\")</li> <li>\"Elements / Formula\" &gt; \"Number of Elements\"</li> </ul> <p>Feel free to incorporate other filters as well. Just try to keep everything in a single view. The more you have to scroll to access the entire dashboard, the more it starts losing its edge. Overall, a dashboard should just provide a quick summary, for more specific filters there are always the side menu and search bar.</p> Tip <p>You have lots of control over the layout of your dashboard. You can shuffle around widgets by click &amp; hold their name and then dragging them around. Expanding their size is done by dragging the bottom-right corner (<code>\u221f</code>). Widgets start out at their minimal default.</p> <p>For a great example of a rich dashboard, visit the app under \"Explore\" &gt; \"Solar Cells\".</p> Success <p>Your dashboard should now look somewhat as in the reference figure. Note that you might have to play around with the layout to get a perfect match. Check the tip box above for more details.</p> <p> </p> <p>Now we can get a quick understanding of what data was uploaded. We are going to re-apply some settings from scenario 2 directly via the dashboard widgets. Restrain the \"Number of Elements\" to 5 and 6 using the slider buttons on the x-axis, and make sure the elements H, C, N, and O are included. As \"Upload Create Time\" updates, only 2 upload times are present now. Switch between selecting one of each upload times. How does the constitution of the data set change? Pay close attention to all the widgets.</p> Success <p>Overall, it seems that the materials covered are quite similar in both. This is not just limited to the composition, but also the crystal makeup. You can verify this yourself by checking the \"Structure\" side pane. The uploads were instead to different datasets, which seem to differ in methodology: GGA vs meta-GGA and hybrid.</p> <p> </p> <p> Final solution </p> <p>What is in a URL?</p> <p>Dashboards can at the moment not be saved between sessions.</p> <p>We should have enough information now to retrieve the paper. While it is nice to have data from a variety of methods, especially for comparison reasons, we are most interested in the HSE06. There seem to be 2 data sets with that tag and we are not sure what the asterisk (<code>*</code>) means. Select both HSE06 datasets then. Click on the top entry in the list. It will fold out, revealing a summary. Find the \"references\" key. Right-button click the DOI hyperlink and open the article in a new tab.</p> <p>Kinds of DOIs</p> <p>In the example above, we see NOMAD linking external DOIs for cross-platform browsing. The same happens with datasets hosted over multiple databases: NOMAD will store the other database's identifier under an \"external id\".</p> <p>Conversely, NOMAD is allowed to issue its own DOIs. Each published dataset receives its own DOI in NOMAD, so it can be cited. You can search for these under \"Author / Origin / Dataset\" &gt; \"Dataset DOI\". Other IDs can be found under \"Visibility / IDs / Schema\".</p> <p>In summary, it is really important to understand whether a DOI (or any other kind of ID) refers to internal or external sources. When in doubt, just hover over the filter or quantity name.</p> Success <p>You should now have Machine learning the quantum-chemical properties of metal\u2013organic frameworks for accelerated materials discovery in front of you. Indeed, next time you bump into your colleague, they will be surprised to learn that you already found it. Actually, any out of the 4 datasets would have brought you to the same paper as well. They are indeed part of the same publication. You can verify this yourself.</p> <p>Reading the paper and the NOMAD dataset side-by-side, can help you get the full context much faster. For example, the abstract mentions that 14.000 experimental MOFs were covered. This is about the size of the PBE dataset (12.600) or both HSE06 sets combined (6.550 each). The discrepancy could be explained away as a rounding error in the text, missing data, or maybe that not each MOF corresponds one-to-one to single calculation. The latter could be case for more complex, composed MOFs. If so, there should be mention of that in the paper.</p> <p>The datasets also show the work process of the authors. They first used a very standard method to sample the materials space. However, GGA is prone to overbinding. Especially in organic systems, the default for a while now has been hybrids, e.g. B3LYP, M066, etc. These are much more expensive in solid state, but still, they ran HSE06 (also a hybrid) over seemingly the entire set. They also experimented with meta-GGAs in about half the cases. The reason therefore can be found in the text. Apparently there is cited work (no. 147) that shows the effectiveness of HLE17 for large band gap prediction in complex materials.</p> <p>Ignoring hidden complexity</p> <p>While the naming of the datasets matches the density functional labels assigned by NOMAD, there could be other relevant information regarding the modelling. A good practice would be check under \"Entry\" &gt; \"DATA\" &gt; \"run\" to find data that has no associated filter, or take a look at the raw input under \"Entry\" &gt; \"FILES\" in case the metadata was not extracted.</p> <p>In this case, the INCAR contains Van der Waals terms (D3 according to the text) as well. This is perfectly normal practice when modeling hydrocarbon chains, but was not picked up on by NOMAD. Support will be added in the future.</p> <p>Meanwhile, the dataset name probably does not reflect these settings, as D3(BJ) is used in all of them. Remember, a dataset name is only as accurate as the author wants it to be.</p> <p>As a last reflection, note how many of the entries and statistics match our findings in scenario 2. Indeed, Andrew Rosen made a big contribution to our coverage of MOFs. Such contributions from the community are what drive NOMAD. You may consider contributing your data already during the research/analysis process, right before submitting a publication, or even later on (especially for data that you have from older publications!).</p> <p>Andrew first even published his data over at figshare{:target=\"blank\"} and shortly after uploaded it to NOMAD. It is good that he did, since NOMAD provides much more information (the full calculation) and covers a wide search range. Meanwhile, over at _figshare, you have to download a zip folder, not knowing what to exactly expect. Andrew clearly put some effort in providing a structured overview, focusing heavily on the MOF description and little else. For the end-user having data available over multiple repositories thus works synergistically, as each platform allows for a different emphasis.</p> <ol> <li> <p>The band gap is the solid state counterpart of the HOMO-LUMO energy gap. Given that NOMAD is materials-centric (while still allowing for molecular systems), it deals in condensed matter nomenclature.  \u21a9</p> </li> <li> <p>The DOS is the solid-state counterpart of the molecular orbital energies. If its sampling extends beyond the HOMO -in solid state, the highest filled energy level is called the Fermi level- it can be used to compute the band gap. This is the case in most simulations.\u00a0\u21a9</p> </li> </ol>"},{"location":"part2/","title":"Part II: How to make an upload and query data in NOMAD.","text":"<p>This part contains the information on how to make an upload with computational data in NOMAD, how to browse through the parsed metadata, and how to use API calls to query data in NOMAD for analysis. We will use an example dataset for a DFT+GW calculation for Si<sub>2</sub>. Similar to the Part I, a more general tutorial can be found in the FAIRmat tutorial 1 and the materials therein.</p> <p>You can find this example data by searching certain parameters, as shown at the end of Part I. You can download the example data files when browsing to the specific entry. The upload id of the example used in this tutorial is 9FVTPztzTdGNVSEbc4QbXg.</p>"},{"location":"part2/#uploading-computational-data-to-nomad","title":"Uploading computational data to NOMAD","text":"<p>Go to the NOMAD Entries page, and click on the top-left menu \"Publish &gt; Uploads\". </p> <p>You can then click on \"Create a new upload\" button, or even try one of the examples in \"Add example uploads\". The page for creating a new upload allows you to upload, edit author metadata, and publish your data with an embargo. The system works by a drag-and-drop files system. For this tutorial, you are going to learn how to upload and explore the metadata of a DFT + GW calculation done by the code exciting. </p>"},{"location":"part2/#browsing-entries","title":"Browsing entries","text":"<p>As you can see in the GIF, after we drag-and-drop the zipped files a processing starts. NOMAD will try to find the corresponding parsers, and if succesful, new entries are generated. In this example, you are uploading a DFT + GW calculation for which three entries are generated: <code>DFT SinglePoint</code>, <code>GW SinglePoint</code>, and <code>GW</code>. You can later visit the Part IV to learn more, but for now is enough to know that NOMAD differentiates between the DFT part, the GW part, and then combines both in what is called the <code>GW workflow</code> entry. This last one is the one containing the combined meta-information of DFT and GW, and thus, it is the most general entry of all three.GW upload gi</p> <p>You can browse each entry by clicking on right arrow <code>\u2192</code>. You will land in the Overview page of the entry, which contains basic information and visualizations for the entry.</p> <p>Extra visualizations, such as the Electronic properties or the Workflow graph will appear depending on the character of the entry (for example, a Molecular Dynamics calculation will not contain Electronic properties but rather Thermodynamic properties and Trajectories visualizations, see FAIRmat tutorial 7).</p> <p> </p> <p>Besides Overview, there are also other menus: \"FILES\", a list of all the files present in the entry folder, \"DATA\", the populated metadata sections and quantities, and \"LOGS\", a list of logging success, warnings and errors. In the \"DATA\" menu, you can browse through the specifically populated NOMAD metainfo definitions.</p> <p>A computational data entry in NOMAD will be composed of the following sections:</p> <ol> <li><code>metadata</code>: contains general meta-information about the entry, such as author, upload time, upload id, etc.</li> <li><code>run</code>: contains all the parsed data from a computational simulation. It divided in four main subsections: <code>program</code> which contains meta-information about the program (name, version...), <code>system</code> which contains meta-information about the system or material for which the calculation was performed (atom positions, symmetry...), <code>method</code> which contains meta-information about the input methodological parameters used in the simulation, and <code>calculation</code> which contains all the output quantities.</li> <li><code>workflow2</code>: contains all the information on the workflow, such as tasks, inputs, outputs, etc. You can learn more in detail about it in Part IV.</li> <li><code>results</code>: contains a set of references to <code>run</code> and <code>workflow2</code> that are then shown in the GUI for visualization and exploration purposes.</li> </ol>"},{"location":"part2/#querying-nomad-data","title":"Querying NOMAD data","text":"<p>In this section, you can learn about how to query processed data in NOMAD by using the Jupyter Notebook prepared specifically to find this entry. This can be found here. Select \"FAIRmat Tutorial 10\" and click \"Start\".</p>"},{"location":"part3/","title":"Part III: Numerical precision in ab initio calculations.","text":"<p>In this part, you will learn about several new quantities and tools for working with numerical-precision settings in NOMAD. These tools will help you</p> <ol> <li>query for the level of precision or conformity that you need.</li> <li>deploy extracted data into a notebook to start performing data science.</li> </ol> <p>Note that precision specifies how close a calculation's convergence is with respect to the complete basis set limit, and not necessarily experiments (that would be accuracy). As such, it only really makes sense when comparing entries of the same system. It is best thought of as a filter that gets applied after you have already chosen your material and method of interest.</p> <p>Since these precision quantities are new and by times community-specific, this tutorial places the emphasis on their definitions. By times it also briefly touches on the bare minimum of theoretical knowledge required to handle them, but mostly leaves the interested reader with references to follow up on. There are also a couple of example instructions guiding you to a specific entry or for downloading processed data into a notebook. Lastly, watch out for the boxes with a pencil sign. They delve deeper into some topics and can be skipped at the first reading.</p>"},{"location":"part3/#lay-out-of-the-precision-section","title":"Lay-out of the Precision section","text":"<p>To start, go to the Entries overview page &gt; FILTERS (side menu) &gt; Precision. To navigate to the Entries page, check out Part I - Exploring NOMAD. The side menu in front of you is ordered so it starts out very general (k-line Density, Code-specific Tier) and below the choice of Basis Set, only contains quantities specific to certain basis set types, e.g. Plane-wave Cutoff, APW Cutoff. The same quantities can be found for each entry in their OVERVIEW page &gt; DATA &gt; results &gt; method &gt; simulation &gt; precision. Following along with the example below, make sure your precision settings are sensible by filtering down to a well-defined system (cubic-centered Actinium) via FILTERS &gt; Material &gt; Ac &gt; only compositions that ... &gt; sorting by Formula.</p> <p>Some quantities are so specific and / or verbose, that they are relegated to DATA. This means that they do not show up in the side menu, nor the search bar. Muffin-tin spheres gives an example of when it is interesting to check them out and how to do so.</p>"},{"location":"part3/#k_section","title":"Reciprocal space","text":"<p>In periodic systems, the most universal numerical parameter is the integration of the reciprocal space, or k-space, and its sampling. With the sampling points often being spaced at fixed intervals, one can define a homogeneous k-density \\(= \\frac{\\text{no. k-points}}{||\\text{k-lattice vector}||}\\). Then, as the k-density ramps up, the Bloch wavefunction converges.  Each (periodic) axis has its own k-density, and though one normally tries to keep these constant among them, fluctuations may happen due the discretized nature of the k-point sampling. To ensure that NOMAD users obtain data that meets their convergence needs, k-line density only shows the lowest density value.</p> <p>Many codes only support 3-D unit cells. Any lower-dimensional cases are then handled by introducing a physical separation (vacuum) between the otherwise periodic images, as well as reducing the k-point sampling to a minimum (1 grid point). Such edge cases can cause false positives, i.e. unphysically low k-line densities, given our current definition. NOMAD distinguishes dimensionality in <code>results.material.toplogy.dimensionality</code> and will therefore only provide a k-line density in a true 3-D case. You may also expect lower-dimensional cases to be supported in the near future, where only periodic axes are accounted for.</p> <p> </p> <p>Band structure calculations</p> <p>For their spatial resolution, band structures sample along line paths connecting several high-symmetry points instead of the reciprocal lattice vectors. Not only can each line path can have its own spacing, their projections onto the reciprocal lattice vectors will vary. In short, they deviate from the fixed spacing requirement, and therefore, NOMAD filters them out. Conceptually, this is fine, since k-line density's aim is provide context to convergence, where this type of sampling does not apply.</p>"},{"location":"part3/#elec_section","title":"Electronic Structure","text":"<p>At the level of the unit cell, there are several paradigms on how to represent the electronic wavefunction. In this tutorial, we explore basis sets that start from plane waves, which mathematically mix well with the Bloch convolution. In particular, we address projector-augmented waves (PAW) and augmented plane waves (APW). As their names suggest, both extend the regular plane waves in regions where convergence is slow, namely around atomic nuclei.</p> <p>A big distinction between PAW and APW is where they each draw this divide:</p> <ul> <li>PAW: the deciding factor is the orbital energies, with those below a certain threshold accounted for in special-purpose pseudopotentials.   More on this in Pseudopotentials.</li> <li>APW: here the divide is spatial in nature.   A (mostly) spherical region, i.e. the muffin-tin sphere, is drawn surrounding the nuclei.   The valence electrons reside in interstitial region between these spheres. </li> </ul>"},{"location":"part3/#val_section","title":"Valence Electrons","text":"<p>The main parameter controlling the plane waves is the longest k-vector \\(\\mathbf{G}^{max}\\). A basis set of plane waves is then generated at fixed intervals, i.e. the secondary parameter, up to \\(\\mathbf{G}^{max}\\). Since the sampling is symmetric in each direction, the length, \\(||\\mathbf{G}^{max}||\\), suffices. By convention, most plane wave (especially PAW) codes express the vector length in energy units, which NOMAD reports as plane-wave cutoff, \\(E^{max}_{cut} = \\frac{\\left(\\hbar ||\\mathbf{G}_{cut}^{max}||\\right)^2}{2m_e}\\).</p> <p> </p> <p>In principle, one can follow the same reasoning for the APW paradigm. However, the cutoff energy alone gives an incomplete view. While in PAW the plane waves sample the whole supercell, in APW they are barred from the muffin-tin spheres. The convention here is to compare the length of \\(G^{max}\\) to the largest muffin-tin radius in reciprocal space, yielding the unit-less fraction, APW cutoff, \\(||\\mathbf{R}_{MT}^{min}|| \\cdot ||\\mathbf{G}_{cut}^{max}||\\).</p> <p> </p> <p>NOMAD points out these differences by specifying the unit after the quantity name and between brackets. For more detail, hover over the quantity name.</p> <p>Both cutoff types can safely be increased to retrieve entries with progressively better converged valence electron wavefunctions.</p> <p>What about grid spacing?</p> <p>At the moment, the mesh of the reciprocal or fast Fourier-transformed (FFT) space is not yet extracted. It is on the planned feature list, though with low priority, considering that most of the convergence is already captured by the cutoff. If an upload or analysis ever requires a new feature, feel free to reach out to us via fairmat@physik.hu-berlin.de.</p>"},{"location":"part3/#core-electrons","title":"Core Electrons","text":"<p>The mathematical parameters describing the electronic core region are extracted, but do not appear in the side menu.  To access them, select an entry, e.g. type <code>entry_id = zxhFQjN5Mny1FW5QEOGxWLPThF3r</code> in the search bar &gt; OVERVIEW &gt; DATA. In the DATA browser follow run (all computational data) &gt; method ( metadata describing the calculation setup) &gt; electrons_representation. This sections contains metadata on the mathematical description of the electronic structure.</p> <p>Each representation comes with a scope and a type, which specify the entity (e.g. wavefunction, density, exchange-correlation density, integration grid) and the overall basis set, respectively. Hardly any code sticks to a single set of parameters values, instead adapting them according to task at hand. For this reason there can be multiple electrons representations, each with their unique scope. Those reported in the search are always the settings for <code>scope = wavefunction</code>.</p> <p>As you have learned in Electronic Structure, some basis sets divide the orbital set into widely different approaches. Each basis_set subsection describes and individual region. Here too, you will find scope and type with pretty much the same definitions. Take note though, since the basis set scope refers to the region it encodes. Examples include cases mentioned above, based on</p> <ul> <li>orbital energy: core vs valence.</li> <li>spatial boundaries: muffin-tin vs interstitial</li> <li>Hamiltonian: kinetic and electron-nucleus interaction vs electron-electron interaction in the case of CP2k's Quickstep algorithm.</li> </ul>"},{"location":"part3/#mt_section","title":"Muffin-tin spheres","text":"<p>APW is an all-electron approach, meaning that all orbitals are relaxed during an electronic self-consistent (SCF) routine. By itself, APW is no longer state-of-the-art and has been followed up by extensions such as LAPW, SLAPW, and APW+lo. Throughout this tutorial, the term APW is used as a shorthand for this entire family of approaches, but in this section alone, it will refer to just the progenitor.</p> <p>While a full overview of the theory is beyond the scope of this tutorial, a quick rundown is necessary to explain some of the NOMAD design choices. The muffin-tin potential around the nucleus \\(\\alpha\\) allows the code to fall back on a more simple, spherically symmetric Hamiltonian to solve. The basis set thus consists out of the harmonics \\(Y_L\\left(r_\\alpha\\right)\\) with radially dependent weights \\(u\\left(r_\\alpha, \\epsilon\\right)\\). In the original APW approach, the energy parameter \\(\\epsilon\\) is a variable that should converge to the respective orbital energies. The core states then align with the well-known case of a multi-electron atom system and are tackled separately from the valence states. <sup>1</sup></p> <p>The muffin-tin valence bands, meanwhile, have to match with their plane-wave counterparts at the boundary. While this formulation of APW is complete, it leads to a set of non-linear equations. More modern implementations linearize these equations by freezing the energy parameter \\(\\epsilon_{l,\\alpha}\\) by nucleus and harmonic index \\(l\\). <sup>1</sup> <sup>2</sup> Obviously, a single \\(\\epsilon_{l,\\alpha}\\) value per \\(l\\)-channel cannot account for dispersion and (anti-)bonding effects, unless they match the state energy perfectly. Instead, corrections are worked in by adding higher-order derivatives of \\(u\\left(r_\\alpha, \\epsilon\\right)\\) to the basis set. Likewise, each derivative order must also match up at the boundary. If the corrections run up to first-order, the approach is called LAPW, else SLAPW for those beyond.</p> <p>Another effect of constraining the energy parameter \\(\\epsilon\\) to \\(l\\), is that it cannot account for wavefunctions with different nodes or main quantum numbers \\(n\\). Some core states, however, are too high-energy to be well-contained within their muffin-tin spheres. They are typically referred to as semicore and have to be treated as valence states. The preferred way to tackle them, is by adding an additional wavefunction, similar to LAPW or SLAPW, but with a normalization requirements in lieu of some boundary constraints. These are called local orbitals (lo) and can be freely added by the user to supplement APW or LAPW. By no means are they restricted to describing semicore states, but can also tackle additional unoccupied states (used in beyond-DFT) as well.</p> <p> </p> <p>While some APW codes require a manual setup for each orbital, e.g. Wien2k, others use a couple of \"steering\" parameters to generate the orbitals. In NOMAD, we typically to keep parameters as concise as possible, though in this case this is not practical for two reasons:</p> <ol> <li>There is no consensus on which steering parameters to use. Each code allows different levels of customization.</li> <li>The energy parameters provided by the user should not be directly used in the SCF routines.    Rather, the code will have some algorithm that optimizes the initial energy parameters based on the geometry.    As such, any orbital degeneracy is lifted.</li> </ol> <p>To capture the orbitals states completely, NOMAD instead \"unrolls\" the steering parameters down to individual radial valence orbitals, identified by their type (e.g. APW, LAPW, lo), associated harmonic index \\(l\\), derivative order of \\(u\\left( r \\right)\\), and of course, the initial energy parameter guess. The sampling grid inside the muffin-tin region, as well as the treatment of the core electrons are all specified at the basis set level.</p> <p>Uploading the right files</p> <p>Most APW codes leave the orbital specification out of their main input file. Similarly, they will also write out their actual initial guesses for the energy parameters out to another intermediate file. If you want NOMAD to pick up on these parameters, please include the files in the table below into your upload. They are automatically generated, so there are no extra steps involved.</p> code name file name exciting <code>&lt;species&gt;.xml</code> fleur <code>out.xml</code> (the main output file) Wien2k <code>&lt;calculation&gt;.inc1</code> Elk Not supported yet"},{"location":"part3/#pseudo_section","title":"Pseudopotentials","text":"<p>With the exception of APW, most plane wave codes hide the core structure via an effective potential for the valence electrons. Where most of these older pseudopotentials where constrained in their range of applications, those built for projector-augmented waves exhibit the highest degree of flexibility.</p> <p>Where before NOMAD would simply indicate the usage of pseudopotentials, it now gives a more complete description, such as: the title, the density functional (or sometimes GW) used to generate the pseudopotential, projector information, the recommended minimum plane wave cutoff to be used for the valence electrons, and whether or not it is norm-conserving. The latter is a useful (but more expensive) integration property and is a prerequisite for some methods.  To try this out for yourself, look up <code>entry_id = zz7J6c9cn_K5B4Ecbiasy4xQ-hYl</code> and navigate to OVERVIEW &gt; DATA &gt; run &gt; method &gt; atom_parameters (describes the calculation setup by elemental type) &gt; pseudopotential.</p> <p>Why am I seeing double quantities?</p> <p>The NOMAD metainfo is semantically constructed. Consequentially, a quantity may belong under several categories. In this case, they always share the same name and definition though. Meanwhile, some quantities (e.g. pseudopotential vs pseudopotential_name) may appear to be similar, but are not exactly identical. Likely, one of them is legacy (pseudopotential_name). The legacy quantity will then be deprecated, though their actual removal may be scheduled later for compatibility reasons.</p> <p>Disclaimer</p> <p>At the moment, parsing is restricted to VASP pseudopotentials, including non-standard or self-made versions. Wider code-support is being built out. NOMAD takes great care in complying with the copyright of the standard POTCAR files and their distribution, so all instances are stripped down to their metadata alone.</p>"},{"location":"part3/#tier_section","title":"Code-specific tiers","text":"<p>Filtering for the aforementioned quantities requires quite some expertise, and it is hard often to weigh the significance of two parameters in the electronic convergence. Some codes have benchmarked their own suggested settings into a list of increasing precision, i.e. tiers. Tiers provide the user base with a relatively safe reference, without having to run any benchmarks themselves. Consequentially, they facilitate standardization and interoperability among users and / or publications. Typically, lower tiers are used to save on resources when exploring large materials' spaces and running large or long simulations. They can also act as starting points for higher tiers that provide high-quality data.</p> <p>Essentially, these tiers hide the high-dimensional parameter values behind a set of hierarchical categories. One such good example of this complexity can be found in <code>entry_id = z-wO_IzCW9sDvysmF500duxvDXDs</code>. The section <code>run.method.electrons_representation</code> contains the corresponding settings. To view all quantities, including those unique to the code, select code specific in the upper-right corner. These are hidden by default.</p> <p>Back in the Entries overview page, you can filter by tier by typing <code>&lt;code name&gt; - &lt;tier name&gt;</code> (e.g. <code>VASP - accurate</code>) into the NOMAD side menu &gt; Precision &gt; Code-specific Tier. <code>&lt;code name&gt;</code> is integral to the format, because tiers are code-specific. This quantity is not meant to provide some kind of comparison across codes. <code>&lt;tier name&gt;</code>, meanwhile, is case-sensitive, but suggestions will pop up once you start typing.</p> <p>Overall, Code-specific tier captures a lot of complexity at once, making it great for quick searches. Beware though: it is also very picky, and will strongly reduce the number of entries returned.</p> <p>Tier matching</p> <p>Contrary to other precision quantities like k-line density, plane-wave cutoff, or APW cutoff, tiers are discrete, not continuous. Settings between two tiers are hard to pin down, even qualitatively, since the setting parameters have differing weights. Therefore, NOMAD only assigns perfect matches. When even one value in the settings is altered by the user, the calculation is immediately disqualified from the tier. This does not mean that the data is invalid or less valuable, though, just that it will be not show up when the filter is applied.</p> <ol> <li> <p>Andris Gulans, Stefan Kontur, Christian Meisenbichler, Dmitrii Nabok, Pasquale Pavone, Santiago Rigamonti, Stephan Sagmeister, Ute Werner, and Claudia Draxl. Exciting: a full-potential all-electron package implementing density-functional theory and many-body perturbation theory. J. Phys.: Condens. Matter, 26(36):363202, September 2014. URL: https://iopscience.iop.org/article/10.1088/0953-8984/26/36/363202 (visited on 2023-04-26), doi:10.1088/0953-8984/26/36/363202.\u00a0\u21a9\u21a9</p> </li> <li> <p>David J. Singh and Lars Nordstr\u00f6m. Planewaves, pseudopotentials, and the LAPW method. Springer, New York, NY, 2nd ed edition, 2006. ISBN 978-0-387-28780-5 978-0-387-29684-5.\u00a0\u21a9</p> </li> </ol>"},{"location":"part5/","title":"Part V: Knowledge-based XC functional exploration","text":"<p>This section was presented as a slide presentation. The PDF version is accessible at  the tutorial's material page.</p>"},{"location":"upload/","title":"Upload","text":"<p>Uploading data in NOMAD can be done in two ways:</p> <ul> <li>By dragging-and-dropping your files into the <code>PUBLISH &gt; Uploads</code> page: suitable for users who have a relatively small amount of data or who want to test how the processing works.</li> <li>By using the Python-based NOMAD API: suitable for users who have larger datasets and need to automatize the upload.</li> <li>By using the shell command <code>curl</code> for sending files to the upload: suitable for users who have larger datasets and need to automatize the upload.</li> </ul> <p>You can upload the files one by one or you can zip them in <code>.zip</code> or <code>.tar.gz</code> formats to upload a larger amount of files at once.</p> <p>We suggest you to visit and read the References &gt; Best Practices: preparing the data and folder structure page to see what are the best practices to organize data in a directory tree prior to upload it.</p>"},{"location":"upload/#drag-and-drop-uploads","title":"Drag-and-drop uploads","text":"<p>On the top-left menu, click on <code>PUBLISH &gt; Uploads</code>.</p> <p>You can then click on <code>CREATE A NEW UPLOAD</code> or try one of the example uploads by clicking in <code>ADD EXAMPLE UPLOADS</code> and selecting one of the multiple options. In our case, we use a zip file with some computational data.</p> <p>You can drag-and-drop your files or click on the <code>CLICK OR DROP FILES</code> button to browse through your local directories.</p> <p>After the files are uploaded, a processing is triggered. Visit Explanation - how the processing works to gain further insight into the process.</p> <p>You will receive an email when the upload processing is finished.</p>"},{"location":"upload/#nomad-api-uploads","title":"NOMAD API uploads","text":"<p>Warning</p> <p>Under construction.</p>"},{"location":"upload/#command-line-uploads","title":"Command-line uploads","text":"<p>Warning</p> <p>Under construction.</p>"},{"location":"upload/#sections-of-the-uploads-page","title":"Sections of the Uploads page","text":"<p>At the top of the uploads page, you can modify certain general metadata fields.</p> <p>The name of the upload can be modify by clicking on the pen icon . The other icons correspond to:</p> <ul> <li> Manage members: allows users to invite collaborators by defining co-authors and reviewers roles.</li> <li> Download files: downloads all files present in the upload.</li> <li> Reload: reloads the uploads page.</li> <li> Reprocess: triggers again the processing of the uploaded data.</li> <li> API: generates a JSON response to use by the NOMAD API. See Querying and performing Data Science for more information.</li> <li> Delete the upload: deletes completely the upload.</li> </ul> <p>The remainder of the uploads page is divided in 4 sections. The first section, (1) Prepare and upload your files, shows the files and folder structure in the upload. You can add a <code>README.md</code> in the root directory and its content will be shown above this section..</p> <p>The second section, (2) Process data, shows the processed data and the generated entries in NOMAD.</p> <p>The third section, (3) Edit author metadata, allows users to edit certain metadata fields from all entries recognized in the upload. This includes comments, where you can add as much extra information as you want, references, where you can add a URL to your upload (e.g., an article DOI), and datasets, where you can create or add the uploaded data into a more general dataset (see How-to publish data &gt; Organizing data in datasets).</p> <p> </p> <p>The final section, (4) Publish, lets the user to publish the data with or without an embargo. This will be explained more in detail in How-to publish data.</p>"},{"location":"Advanced/AI_toolkit/","title":"The AI Toolkit","text":"<p>The Artificial-Intelligence Toolkit is a collection of tools and tutorials for applying artificial-intelligence approaches (including machine-learning, compressed sensing, and data mining) to the materials data found on the NOMAD repository.</p> <p>Currently, the AI-toolkit focuses on analysis of Ab Initio data. This focus will be broadened as a larger amount of MD data is added to NOMAD. In the mean time, the toolkit still provides a nice platform for learning about state-of-the-art data analysis techniques in the context of materials data.</p>"},{"location":"Advanced/Upload_API/","title":"Uploading, changing metadata, and publishing via python API","text":"<p>The NOMAD API allows uploading, publishing, etc. using a local python environment, as an alternative to the NOMAD GUI.</p> <p>We have prepare some simple python functions to facilitate use of this API. For use as demonstrated below, copy the following code into a file called NOMAD_API.py:</p> <pre><code>import requests\ndef get_authentication_token(nomad_url, username, password):\n'''Get the token for accessing your NOMAD unpublished uploads remotely'''\ntry:\nresponse = requests.get(\nnomad_url + 'auth/token', params=dict(username=username, password=password), timeout=10)\ntoken = response.json().get('access_token')\nif token:\nreturn token\nprint('response is missing token: ')\nprint(response.json())\nreturn\nexcept Exception:\nprint('something went wrong trying to get authentication token')\nreturn\ndef create_dataset(nomad_url, token, dataset_name):\n'''Create a dataset to group a series of NOMAD entries'''\ntry:\nresponse = requests.post(\nnomad_url + 'datasets/',\nheaders={'Authorization': f'Bearer {token}', 'Accept': 'application/json'},\njson={\"dataset_name\": dataset_name},\ntimeout=10\n)\ndataset_id = response.json().get('dataset_id')\nif dataset_id:\nreturn dataset_id\nprint('response is missing dataset_id: ')\nprint(response.json())\nreturn\nexcept Exception:\nprint('something went wrong trying to create a dataset')\nreturn\ndef upload_to_NOMAD(nomad_url, token, upload_file):\n'''Upload a single file for NOMAD upload, e.g., zip format'''\nwith open(upload_file, 'rb') as f:\ntry:\nresponse = requests.post(\nnomad_url + 'uploads',\nheaders={'Authorization': f'Bearer {token}', 'Accept': 'application/json'},\ndata=f, timeout=30)\nupload_id = response.json().get('upload_id')\nif upload_id:\nreturn upload_id\nprint('response is missing upload_id: ')\nprint(response.json())\nreturn\nexcept Exception:\nprint('something went wrong uploading to NOMAD')\nreturn\ndef check_upload_status(nomad_url, token, upload_id):\n'''\n    # upload success =&gt; returns 'Process publish_upload completed successfully'\n    # publish success =&gt; 'Process publish_upload completed successfully'\n    '''\ntry:\nresponse = requests.get(\nnomad_url + 'uploads/' + upload_id,\nheaders={'Authorization': f'Bearer {token}'}, timeout=30)\nstatus_message = response.json().get('data').get('last_status_message')\nif status_message:\nreturn status_message\nprint('response is missing status_message: ')\nprint(response.json())\nreturn\nexcept Exception:\nprint('something went wrong trying to check the status of upload' + upload_id)\n# upload gets deleted from the upload staging area once published...or in this case something went wrong\nreturn\ndef edit_upload_metadata(nomad_url, token, upload_id, metadata):\n'''\n    Example of new metadata:\n    upload_name = 'Test_Upload_Name'\n    metadata = {\n        \"metadata\": {\n        \"upload_name\": upload_name,\n        \"references\": [\"https://doi.org/10.xxxx/xxxxxx\", \"http://doi.org/10.xxxx/xxxxx\"],\n        \"datasets\": dataset_id,\n        \"embargo_length\": 0,\n        \"coauthors\": [\"coauthor@test.de\"],\n        \"comment\": 'This is a test upload...'\n        },\n    }\n    '''\ntry:\nresponse = requests.post(\nnomad_url+'uploads/' + upload_id + '/edit',\nheaders={'Authorization': f'Bearer {token}', 'Accept': 'application/json'},\njson=metadata, timeout=30)\nreturn response\nexcept Exception:\nprint('something went wrong trying to add metadata to upload' + upload_id)\nreturn\ndef publish_upload(nomad_url, token, upload_id):\n'''Publish an upload'''\ntry:\nresponse = requests.post(\nnomad_url+'uploads/' + upload_id + '/action/publish',\nheaders={'Authorization': f'Bearer {token}', 'Accept': 'application/json'},\ntimeout=30)\nreturn response\nexcept Exception:\nprint('something went wrong trying to publish upload: ' + upload_id)\nreturn\n</code></pre> <p>Now, we will demonstrate how to use these functions. Within a notebook or python script, import the above functions:</p> <pre><code>from Nomad_API import *`\n</code></pre> <p>Define the following user information: <pre><code>username = 'nomad_email@affiliation.edu'\npassword = 'password'\n</code></pre></p> <p>Define the NOMAD API endpoint: <pre><code># nomad_url = 'https://nomad-lab.eu/prod/v1/api/v1/'  # production nomad\nnomad_url = 'https://nomad-lab.eu/prod/v1/test/api/v1/'  # test nomad (deleted occassionally)\n</code></pre></p> <p>Get a token for accessing your unpublished uploads:</p> <pre><code>token = get_authentication_token(nomad_url, username, password)\n</code></pre> <p>Create a dataset for grouping uploads that belong to, e.g., a publication:</p> <pre><code>dataset_id = create_dataset(nomad_url, token, 'Test_Dataset')\n</code></pre> <p>Upload some test data to NOMAD:</p> <pre><code>upload_id = upload_to_NOMAD(nomad_url, token, 'test_data.zip')\n</code></pre> <p>Check the status to make sure the upload was processed correctly:</p> <pre><code>last_status_message = check_upload_status(nomad_url, token, upload_id)\nprint(last_status_message)\n</code></pre> <p>The immediate result may be:</p> <pre><code>'Waiting for results (level 0)'\n</code></pre> <p>After some time you will get:</p> <pre><code>'Process process_upload completed successfully'\n</code></pre> Tip <p>If your upload contains a reasonably sized system or trajectory, you may have to wait some time before the processing finishing. You can call this function intermittantly, e.g., in a while loop with a sleep call in between, waiting for <code>last_status_message</code> to be \"Process process_upload completed successfully\"</p> <p>Now that the upload processing is complete, we can add coauthors, references, and other comments, as well as link to a dataset and provide a proper name for the upload:</p> <pre><code>metadata = {\n\"metadata\": {\n\"upload_name\": 'Test_Upload',\n\"references\": [\"https://doi.org/10.1063/5.0104914\", \"http://doi.org/10.5281/zenodo.6032826\"],\n\"datasets\": dataset_id,\n\"embargo_length\": 0,\n\"coauthors\": [\"bereau@thphys.uni-heidelberg.de\"],\n\"comment\": 'Test.',\n},\n}\nresponse = edit_upload_metadata(nomad_url, token, upload_id, metadata)\n</code></pre> <p>Check the upload again to make sure that the metadata was changed:</p> <pre><code>last_status_message = check_upload_status(nomad_url, token, upload_id)\nprint(last_status_message)\n</code></pre> <pre><code>'Process edit_upload_metadata completed successfully'\n</code></pre> <p>Now, we are ready to publish:</p> <pre><code>response = publish_upload(nomad_url, token, upload_id)\n</code></pre> <p>Once again check the status:</p> <pre><code>last_status_message = check_upload_status(nomad_url, token, upload_id)\nprint(last_status_message)\n</code></pre> <pre><code>'Process publish_upload completed successfully'\n</code></pre>"},{"location":"Advanced/part4/","title":"Workflows and how to link DFT and beyond-DFT calculations","text":"<p>This part contains the basic knowledge on understanding and learning to use NOMAD workflows, and its relation with DFT and beyond-DFT (GW, BSE, DMFT, etc.) methodologies. You will use a fictitious example of a simulation workflow with the following files and folder structure: <pre><code>.\n\u251c\u2500\u2500 pressure1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t1.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t2.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dft_p1.xml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tb_p1.wout\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n\u2514\u2500\u2500 pressure2\n \u00a0\u00a0 \u251c\u2500\u2500 temperature1\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t1.hdf5\n \u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n \u00a0\u00a0 \u251c\u2500\u2500 temperature2\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t2.hdf5\n \u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n \u00a0\u00a0 \u251c\u2500\u2500 dft_p2.xml\n \u00a0\u00a0 \u251c\u2500\u2500 tb_p2.wout\n \u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n</code></pre></p> <p>which can be downloaded here:  Download example_files.zip </p> <p>Each of the mainfiles represent an electronic-structure calculation (either DFT{:target=\"blank\"}, TB, or DMFT) which in turn is then parsed into a singular _entry in NOMAD. When dragged into the NOMAD Upload page{:target=\"blank\"}, these files should generate 8 entries in total. This folder structure presents a typical workflow calculation which can be represented as a provenance graph: <pre><code>graph LR;\n    A2((Inputs)) --&gt; B2[DFT];\n    A1((Inputs)) --&gt; B1[DFT];\n    subgraph pressure P&lt;sub&gt;2&lt;/sub&gt;\n    B2[DFT] --&gt; C2[TB];\n    C2[TB] --&gt; D21[DMFT at T&lt;sub&gt;1&lt;/sub&gt;];\n    C2[TB] --&gt; D22[DMFT at T&lt;sub&gt;2&lt;/sub&gt;];\n    end\n    D21[DMFT at T&lt;sub&gt;1&lt;/sub&gt;] --&gt; E21([Output calculation P&lt;sub&gt;2&lt;/sub&gt;, T&lt;sub&gt;1&lt;/sub&gt;])\n    D22[DMFT at T&lt;sub&gt;2&lt;/sub&gt;] --&gt; E22([Output calculation P&lt;sub&gt;2&lt;/sub&gt;, T&lt;sub&gt;2&lt;/sub&gt;])\n    subgraph pressure P&lt;sub&gt;1&lt;/sub&gt;\n    B1[DFT] --&gt; C1[TB];\n    C1[TB] --&gt; D11[DMFT at T&lt;sub&gt;1&lt;/sub&gt;];\n    C1[TB] --&gt; D12[DMFT at T&lt;sub&gt;2&lt;/sub&gt;];\n    end\n    D11[DMFT at T&lt;sub&gt;1&lt;/sub&gt;] --&gt; E11([Output calculation P&lt;sub&gt;1&lt;/sub&gt;, T&lt;sub&gt;1&lt;/sub&gt;])\n    D12[DMFT at T&lt;sub&gt;2&lt;/sub&gt;] --&gt; E12([Output calculation P&lt;sub&gt;1&lt;/sub&gt;, T&lt;sub&gt;2&lt;/sub&gt;])</code></pre> Here, \"Input\" refers to the all _input information given to perform the calculation (e.g., atom positions, model parameters, experimental initial conditions, etc.). \"DFT\", \"TB\" and \"DMFT\" refer to individual tasks of the workflow, which each correspond to a SinglePoint entry in NOMAD. \"Output calculation\" refers to the output data of each of the final DMFT tasks.</p> <p>The goal of this part is to set up the following workflows:</p> <ol> <li>A <code>SinglePoint</code> workflow for one of the calculations (e.g., the DFT one) in the <code>pressure1</code> subfolder.</li> <li>An overarching workflow entry for each pressure P<sub>i=1,2</sub>, grouping all <code>SinglePoint</code> \"DFT\", \"TB\", \"DMFT at T<sub>1</sub>\", and \"DMFT at T<sub>2</sub>\" tasks.</li> <li>A top level workflow entry, grouping together all pressure calculations.</li> </ol> <p>The files for all these cases can be downloaded here:  Download worfklowyaml_files.zip </p> <p>You can try writing these files yourself first, and then compare them with the tested files.</p>"},{"location":"Advanced/part4/#starting-example-singlepoint-workflow","title":"Starting example: SinglePoint workflow","text":"<p>NOMAD is able to recognize certain workflows in an automatic way, such as the <code>SinglePoint</code> case mentioned above. However, to showcase how to the use workflows in NOMAD, you will learn how to \"manually\" construct the SinglePoint workflow, represented by the following provenance graph: <pre><code>graph LR;\n    A((Inputs)) --&gt; B[DFT];\n    B[DFT] --&gt; C([Output calculation]);</code></pre> To define a workflow manually in NOMAD, you must add a YAML file to the upload folder that contains the relevant input, output, and task information. This file should be named <code>&lt;filename&gt;.archive.yaml</code>. In this case, you should include the file <code>single_point.archive.yaml</code> with the following content:</p> <pre><code>workflow2:\nname: SinglePoint\ninputs:\n- name: Input structure\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\noutputs:\n- name: Output calculation\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\ntasks:\n- m_def: nomad.datamodel.metainfo.workflow.TaskReference\ntask: '../upload/archive/mainfile/pressure1/dft_p1.xml#/workflow2'\nname: DFT at Pressure P1\ninputs:\n- name: Input structure\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\noutputs:\n- name: Output calculation\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n</code></pre> <p>Note several things about the content of this file:</p> <ol> <li><code>name</code> keys are optional.</li> <li>The root path of the upload can be referenced with <code>../upload/archive/mainfile/</code>. Starting from there, the original directory tree structure of the upload is maintained.</li> <li><code>inputs</code> reference the section containing inputs of the whole workflow. In this case this is the section <code>run[0].system[-1]</code> parsed from the mainfile in the path <code>pressure1/dft_p1.xml</code>.</li> <li><code>outputs</code> reference the section containing outputs of the whole workflow. In this case this is the section <code>run[0].calculation[-1]</code> parsed from the mainfile in the path <code>pressure1/dft_p1.xml</code>.</li> <li><code>tasks</code> reference the section containing tasks of each step in the workflow. These must also contain <code>inputs</code> and <code>outputs</code> properly referencing the corresponding sections; this will then link inputs/outputs/tasks in the NOMAD Archive. In this case this is a <code>TaskReference</code> to the section <code>workflow2</code> parsed from the mainfile in the path <code>pressure1/dft_p1.xml</code>.</li> <li><code>section</code> reference to the uploaded mainfile specific section. The left side of the <code>#</code> symbol contains the path to the mainfile, while the right contains the path to the section.</li> </ol> <p>This will produce an extra entry with the following Overview content:</p> <p>Note that you are referencing sections which are lists. Thus, in each case you should be careful to reference the correct section for inputs and outputs (example: a <code>GeometryOptimization</code> workflow calculation will have the \"Input structure\" as <code>run[0].system[0]</code>, while the \"Output calculation\" would also contain <code>run[0].system[-1]</code>, and all intermediate steps must input/output the corresponding section system).</p> <p>NOMAD workflow filename</p> <p>The NOMAD workflow YAML file name, i.e., <code>&lt;filename&gt;</code> in the explanation above, can be any custom name defined by the user, but the file must keep the extension <code>.archive.yaml</code> at the end. This is done in order for NOMAD to recognize this file as a custom schema. Custom schemas are widely used in experimental parsing, and you can learn more about them in the FAIRmat tutorial 8.</p> <p>You can extend the workflow meta-information by adding the metholodogical input parameters. These are stored in NOMAD in the section path <code>run[0].method[-1]</code>. The new <code>single_point.archive.yaml</code> will be:</p> <pre><code>workflow2:\nname: SinglePoint\ninputs:\n- name: Input structure\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n- name: Input methodology parameters\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/method/-1'\noutputs:\n- name: Output calculation\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\ntasks:\n- m_def: nomad.datamodel.metainfo.workflow.TaskReference\ntask: '../upload/archive/mainfile/pressure1/dft_p1.xml#/workflow2'\nname: DFT at Pressure P1\ninputs:\n- name: Input structure\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n- name: Input methodology parameters\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/method/-1'\noutputs:\n- name: Output calculation\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n</code></pre> <p>which in turn produces a similar workflow than before, but with an extra input node:</p>"},{"location":"Advanced/part4/#pressure-workflows","title":"Pressure workflows","text":"<p>Now that you know the basics of the workflow YAML schema, let's try to define an overarching workflow for each of the pressures. For this section, you will learn how to create the workflow YAML schema for the P<sub>1</sub> case; the extension for P<sub>2</sub> is then a matter of changing names and paths in the YAML files. For simplicity, you can skip referencing to methodologies.</p> <p>Thus, the <code>inputs</code> can be defined as: <pre><code>workflow2:\nname: DFT+TB+DMFT at P1\ninputs:\n- name: Input structure\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n</code></pre> and there are two <code>outputs</code>, one for each of the DMFT calculations at distinct temperatures: <pre><code>  outputs:\n- name: Output DMFT at P1, T1 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n- name: Output DMFT at P1, T2 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n</code></pre> Now, <code>tasks</code> are defined for each of the methodologies performed (each corresponding to an underlying SinglePoint workflow). To define a valid workflow, each task must contain an input that corresponds to one of the outputs of the previous task. Moreover, the first task should take as input the overall input of the workflow, and the final task should also have as an output the overall workflow output. Then: <pre><code>  tasks:\n- m_def: nomad.datamodel.metainfo.workflow.TaskReference\ntask: '../upload/archive/mainfile/pressure1/dft_p1.xml#/workflow2'\nname: DFT at P1\ninputs:\n- name: Input structure\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\noutputs:\n- name: Output DFT at P1 calculation\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n- m_def: nomad.datamodel.metainfo.workflow.TaskReference\ntask: '../upload/archive/mainfile/pressure1/tb_p1.wout#/workflow2'\nname: TB at P1\ninputs:\n- name: Input DFT at P1 calculation\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\noutputs:\n- name: Output TB at P1 calculation\nsection: '../upload/archive/mainfile/pressure1/tb_p1.wout#/run/0/calculation/-1'\n- m_def: nomad.datamodel.metainfo.workflow.TaskReference\ntask: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/workflow2'\nname: DMFT at P1 and T1\ninputs:\n- name: Input TB at P1 calculation\nsection: '../upload/archive/mainfile/pressure1/tb_p1.wout#/run/0/calculation/-1'\noutputs:\n- name: Output DMFT at P1, T1 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n- m_def: nomad.datamodel.metainfo.workflow.TaskReference\ntask: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/workflow2'\nname: DMFT at P1 and T2\ninputs:\n- name: Input TB at P1 calculation\nsection: '../upload/archive/mainfile/pressure1/tb_p1.wout#/run/0/calculation/-1'\noutputs:\n- name: Output DMFT at P1, T2 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n</code></pre> Note here:</p> <ul> <li>The <code>inputs</code> for each subsequent step are the <code>outputs</code> of the previous step.</li> <li>The final two <code>outputs</code> coincide with the <code>workflow2</code> <code>outputs</code>.</li> </ul> <p>This workflow (<code>pressure1.archive.yaml</code>) file will then produce an entry with the following Overview page:</p> <p>Similarly, for P<sub>2</sub> you can upload a new <code>pressure2.archive.yaml</code> file with the same content, except when substituting 'pressure1' and 'p1' by their counterparts. This will produce a similar graph than the one showed before but for \"P2\".</p>"},{"location":"Advanced/part4/#the-top-level-workflow","title":"The top-level workflow","text":"<p>After adding the workflow YAML files, Your upload folder directory now looks like: <pre><code>.\n\u251c\u2500\u2500 pressure1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t1.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t2.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dft_p1.xml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tb_p1.wout\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n\u251c\u2500\u2500 pressure1.archive.yaml\n\u251c\u2500\u2500 pressure2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t1.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t2.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dft_p2.xml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tb_p2.wout\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n\u251c\u2500\u2500 pressure2.archive.yaml\n\u2514\u2500\u2500 single_point.archive.yaml\n</code></pre> In order to define the general workflow that groups all pressure calculations, YOU can reference directly the previous <code>pressureX.archive.yaml</code> files as tasks. Still, <code>inputs</code> and <code>outputs</code> must be referenced to their corresponding mainfile and section paths.</p> <p>Create a new <code>fullworkflow.archive.yaml</code> file with the <code>inputs</code>: <pre><code>workflow2:\nname: Full calculation at different pressures for SrVO3\ninputs:\n- name: Input structure at P1\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n- name: Input structure at P2\nsection: '../upload/archive/mainfile/pressure2/dft_p2.xml#/run/0/system/-1'\n</code></pre> And <code>outputs</code>: <pre><code>  outputs:\n- name: Output DMFT at P1, T1 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n- name: Output DMFT at P1, T2 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n- name: Output DMFT at P2, T1 calculation\nsection: '../upload/archive/mainfile/pressure2/temperature1/dmft_p2_t1.hdf5#/run/0/calculation/-1'\n- name: Output DMFT at P2, T2 calculation\nsection: '../upload/archive/mainfile/pressure2/temperature2/dmft_p2_t2.hdf5#/run/0/calculation/-1'\n</code></pre> Finally, <code>tasks</code> references the previous YAML schemas as follows: <pre><code>  tasks:\n- m_def: nomad.datamodel.metainfo.workflow.TaskReference\ntask: '../upload/archive/mainfile/pressure1.archive.yaml#/workflow2'\nname: DFT+TB+DMFT at P1\ninputs:\n- name: Input structure at P1\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\noutputs:\n- name: Output DMFT at P1, T1 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n- name: Output DMFT at P1, T2 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n- m_def: nomad.datamodel.metainfo.workflow.TaskReference\ntask: '../upload/archive/mainfile/pressure2.archive.yaml#/workflow2'\nname: DFT+TB+DMFT at P2\ninputs:\n- name: Input structure at P2\nsection: '../upload/archive/mainfile/pressure2/dft_p2.xml#/run/0/system/-1'\noutputs:\n- name: Output DMFT at P2, T1 calculation\nsection: '../upload/archive/mainfile/pressure2/temperature1/dmft_p2_t1.hdf5#/run/0/calculation/-1'\n- name: Output DMFT at P2, T2 calculation\nsection: '../upload/archive/mainfile/pressure2/temperature2/dmft_p2_t2.hdf5#/run/0/calculation/-1'\n</code></pre></p> <p>This will produce the following entry and its Overview page:</p>"},{"location":"Advanced/part4/#automatic-workflows","title":"Automatic workflows","text":"<p>There are some cases where the NOMAD infrastructure is able to recognize certain workflows automatically when processing the uploaded files. The simplest example is any <code>SinglePoint</code> calculation, as explained above. Other examples include <code>GeometryOptimization</code>, <code>Phonons</code>, <code>GW</code>, and <code>MolecularDynamics</code>. Automated workflow detection may require your folder structure to fulfill certain conditions.</p> <p>Here are some general guidelines for preparing your upload folder in order to make it easier for the automatic workflow recognition to work:</p> <ul> <li>Always organize your files in an top-down structure, i.e., the initial tasks should be upper in the directory tree, while the later tasks lower on it.</li> <li>Avoid having to go up and down between folders if some properties are derived between these files. These situations are very complicated to predict for the current NOMAD infrastructure.</li> <li>Avoid duplication of files in subfolders. If initially you do a calculation A from which a later calculation B is derived and you want to store B in a subfolder, there is no need to copy the A files inside the subfolder B.</li> </ul> <p>The folder structure used throughout this part is a good example of a clean upload which is friendly and easy to work with when defining NOMAD workflows.</p>"}]}